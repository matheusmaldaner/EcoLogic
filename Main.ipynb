{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df65bb0b-594d-4717-bd3c-63b04b9d864b",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fb5dd1c0-73cf-4679-ae21-6fa00a85a395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import yaml\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Compose, Lambda\n",
    "import mnist_dataset\n",
    "from hydra import initialize, compose\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, Subset\n",
    "from difflogic import LogicLayer, GroupSum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99c41f-b5c7-4d64-a704-5a003a5b56c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8096d-fc17-46d7-861e-685d2fe12dbb",
   "metadata": {},
   "source": [
    "Tunable Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "47ea8409-a084-4bee-8121-96acd99bb28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configurable options\n",
    "remove_border = True       # True: Removes border of Mnist, False: Keeps black border around digits\n",
    "binarize_images = True     # True: Binarized Images, False: Grayscale Images \n",
    "evenly_partitioned = True  # True: Even distribution of samples, False: Original Mnist distribution\n",
    "upscaled_images = False    # True: Upscales the samples to 32x32, False: Keeps size unchanged\n",
    "downscaled_images = True   # True: Downscales the samples to 16x16, False: Keeps size unchanged\n",
    "batch_size = 256           # Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4450630-e7a8-4564-b106-6e2dc3a12e61",
   "metadata": {},
   "source": [
    "Dataset Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1779afcd-2def-41d1-abdc-c1bc8dbb1719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to binarize an image, threshold is tunable \n",
    "def binarize(image, threshold=0.5):\n",
    "    return (image > threshold).float()  \n",
    "\n",
    "# define the transformation logic based on the toggle\n",
    "transform_list = [ToTensor()]\n",
    "\n",
    "if upscaled_images:\n",
    "    transform_list.append(Resize((32, 32)))\n",
    "elif downscaled_images:\n",
    "    transform_list.append(Resize((16, 16)))\n",
    "if binarize_images:\n",
    "    transform_list.append(Lambda(lambda x: binarize(x)))\n",
    "    \n",
    "# adds binarization if enabled\n",
    "if binarize_images:\n",
    "    transform_list.append(Lambda(lambda x: binarize(x)))\n",
    "    \n",
    "transform = Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "9d9255d2-bf0c-4bab-b66a-b65360bcb86f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = mnist_dataset.MNIST('./data-mnist', train=True, download=True, remove_border=remove_border, transform=transform)\n",
    "test_dataset = mnist_dataset.MNIST('./data-mnist', train=False, remove_border=remove_border, transform=transform)\n",
    "\n",
    "# drop_last = True means it will drop the last incomplete Batch\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "38a5db70-d2ba-44a7-803c-829177a182c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes the Dataset evenly partitioned\n",
    "if evenly_partitioned:\n",
    "    # code below is used so that all classes have the same number of samples\n",
    "    train_targets = train_loader.dataset.targets\n",
    "    test_targets = test_loader.dataset.targets\n",
    "\n",
    "    train_digits_total = []\n",
    "    test_digits_total = []\n",
    "\n",
    "    for i in range(10):\n",
    "        curr_tot_train = torch.sum(train_targets == i).item()\n",
    "        curr_tot_test = torch.sum(test_targets == i).item()    \n",
    "        train_digits_total.append(curr_tot_train)\n",
    "        test_digits_total.append(curr_tot_test)\n",
    "\n",
    "    train_digits_total, test_digits_total\n",
    "\n",
    "    # find the minimum number of samples across all classes\n",
    "    min_samples_train = min(train_digits_total)\n",
    "    min_samples_test = min(test_digits_total)\n",
    "\n",
    "    # function to trim dataset to match the minimum samples for each class and shuffle indices\n",
    "    def trim_dataset(dataset, targets, min_samples):\n",
    "        indices = []\n",
    "        for i in range(10):\n",
    "            class_indices = (targets == i).nonzero(as_tuple=True)[0]  # get indices of class i\n",
    "            class_indices = class_indices[:min_samples]  # trim to min_samples\n",
    "            indices.extend(class_indices)\n",
    "\n",
    "        # shuffle indices after collecting them\n",
    "        indices = torch.tensor(indices)\n",
    "        indices = indices[torch.randperm(indices.size(0))]  \n",
    "\n",
    "        return Subset(dataset, indices)\n",
    "\n",
    "    # trim both train and test datasets to ensure all classes have the same number of samples\n",
    "    trimmed_train_dataset = trim_dataset(train_loader.dataset, train_targets, min_samples_train)\n",
    "    trimmed_test_dataset = trim_dataset(test_loader.dataset, test_targets, min_samples_test)\n",
    "\n",
    "    # create DataLoaders for the trimmed datasets\n",
    "    trimmed_train_loader = DataLoader(trimmed_train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    trimmed_test_loader = DataLoader(trimmed_test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "\n",
    "    # verify the lengths of the trimmed datasets\n",
    "    len(trimmed_train_loader.dataset), len(trimmed_test_loader.dataset)\n",
    "\n",
    "    train_dataset = trimmed_train_dataset\n",
    "    test_dataset = trimmed_test_dataset\n",
    "    train_loader = trimmed_train_loader\n",
    "    test_loader = trimmed_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "aebb9b7f-2399-4b9f-8a8a-570c922f92ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54210, 8920)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = len(train_loader.dataset) + len(test_loader.dataset)\n",
    "len(train_loader.dataset), len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "66230413-408e-4bc1-bf5f-2313690e0964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "(3, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[201], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m random_index \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, dataset_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_border:\n\u001b[0;32m----> 5\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrandom_index\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remove_border \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m upscaled_images \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m downscaled_images:\n\u001b[1;32m      7\u001b[0m     image \u001b[38;5;241m=\u001b[39m train_loader\u001b[38;5;241m.\u001b[39mdataset[random_index][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n",
      "File \u001b[0;32m/blue/woodard/mkunzlermaldaner/project/SKEPTIC/schoolhouse_ex/X_StateEmbedding/env/difflogic_env/lib/python3.9/site-packages/torch/utils/data/dataset.py:311\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/blue/woodard/mkunzlermaldaner/project/EcoLogic/mnist_dataset.py:194\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    191\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 194\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/blue/woodard/mkunzlermaldaner/project/SKEPTIC/schoolhouse_ex/X_StateEmbedding/env/difflogic_env/lib/python3.9/site-packages/torchvision/transforms/transforms.py:60\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 60\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/blue/woodard/mkunzlermaldaner/project/EcoLogic/mnist_dataset.py:25\u001b[0m, in \u001b[0;36mMNISTRemoveBorderTransform.__call__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m horizontal_black_lines[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39mbottom_black \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m bottom_black \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m14\u001b[39m:\n\u001b[1;32m     24\u001b[0m     bottom_black \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m top_black \u001b[38;5;241m+\u001b[39m bottom_black \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m, (top_black, bottom_black)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m top_black \u001b[38;5;241m+\u001b[39m bottom_black \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m top_black \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: (3, 1)"
     ]
    }
   ],
   "source": [
    "dataset_size = len(train_dataset)\n",
    "random_index = random.randint(0, dataset_size - 1)\n",
    "\n",
    "if remove_border:\n",
    "    image = train_loader.dataset[random_index][0].reshape(20, 20)\n",
    "elif not remove_border and not upscaled_images and not downscaled_images:\n",
    "    image = train_loader.dataset[random_index][0].reshape(28, 28)\n",
    "elif downscaled_images:\n",
    "    image = train_loader.dataset[random_index][0].reshape(16, 16)\n",
    "else:\n",
    "    image = train_loader.dataset[random_index][0].reshape(32, 32)\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973f0e3-5f32-4793-afc6-9394425419de",
   "metadata": {},
   "source": [
    "#### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e63be-c489-4aaf-a605-9e1d0fda631f",
   "metadata": {},
   "source": [
    "Converts csv into yaml config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9b89fd2-f200-40c1-973a-489ef4190e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define first input and the name of the file to be saved\n",
    "first_in_dim = 256 # 16x16\n",
    "filename = \"config/mnist_config_16x16.yaml\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dc64546-f1d7-4e18-b88c-7140d4d42692",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML file 'config/mnist_config_16x16.yaml' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# reads the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"config/hyperparameters.csv\")\n",
    "\n",
    "# convert the DataFrame to a list of dictionaries\n",
    "models = df.to_dict(orient=\"records\")\n",
    "\n",
    "# create the YAML structure\n",
    "yaml_structure = {\"models\": {}}\n",
    "\n",
    "# rounds the number to the nearest multiple of the output size\n",
    "def round_to_nearest_multiple(value, multiple):\n",
    "    return multiple * round(value / multiple)\n",
    "\n",
    "# populate the YAML structure with models\n",
    "for i, model in enumerate(models, start=1):\n",
    "    # zero-padding model names to 3 digits \n",
    "    model_name = f\"model_{str(i).zfill(3)}\"\n",
    "    layers_config = {}\n",
    "    \n",
    "    for layer in range(1, model[\"H\"] + 1):\n",
    "        # zero-padding the layer names to 3 digits\n",
    "        layer_name = f\"LogicLayer{str(layer).zfill(3)}\"\n",
    "        \n",
    "        # adjusts in_dim to the nearest multiple of 10\n",
    "        in_dim = first_in_dim if layer == 1 else round_to_nearest_multiple(model[\"W\"], 10)\n",
    "        \n",
    "        # adjusts out_dim to the nearest multiple of 10\n",
    "        out_dim = round_to_nearest_multiple(model[\"W\"], 10)\n",
    "        \n",
    "        layers_config[layer_name] = {\n",
    "            \"in_dim\": in_dim,\n",
    "            \"out_dim\": out_dim,\n",
    "            \"device\": \"cuda\",\n",
    "            \"implementation\": \"cuda\",\n",
    "            \"connections\": \"random\",\n",
    "            \"grad_factor\": 2, # we can try different grad_factor values as well\n",
    "        }\n",
    "    \n",
    "    yaml_structure[\"models\"][model_name] = {\n",
    "        \"input_dim\": first_in_dim, \n",
    "        \"output_size\": 10, # for MNIST classification\n",
    "        \"tau\": model[\"tau\"],\n",
    "        \"learning_rate\": model[\"lr\"],\n",
    "        \"layers_config\": layers_config,\n",
    "    }\n",
    "\n",
    "# saves to a YAML file\n",
    "with open(f'{filename}', \"w\") as file:\n",
    "    yaml.dump(yaml_structure, file, default_flow_style=False)\n",
    "\n",
    "print(f\"YAML file '{filename}' generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e939ace-e4c3-4781-bdea-f9722ddb925e",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473370c-ec74-455b-854c-f597d04144f3",
   "metadata": {},
   "source": [
    "DiffLogic Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b244edec-9bd5-474d-9aaa-4f9aafe31b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffLogic(nn.Module):\n",
    "    def __init__(self, layers_config, output_size, tau=30):\n",
    "        \"\"\"\n",
    "        Initializes the DiffLogic model with the specified layer configurations, output size, and temperature parameter.\n",
    "\n",
    "        Args:\n",
    "            layers_config (dict): Configuration for each logic layer, including dimensions, device, implementation, connections, and grad factor.\n",
    "            output_size (int): The number of output groups (classes in a classification problem).\n",
    "            tau (int): Temperature parameter for the GroupSum operation.\n",
    "        \"\"\"\n",
    "        super(DiffLogic, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # stores the logic layers\n",
    "        layers = []\n",
    "        for layer_name, config in layers_config.items():\n",
    "            layer = LogicLayer(\n",
    "                in_dim=config['in_dim'],\n",
    "                out_dim=config['out_dim'],\n",
    "                device=config['device'],\n",
    "                implementation=config['implementation'],\n",
    "                connections=config['connections'],\n",
    "                grad_factor=config['grad_factor']       \n",
    "            )\n",
    "            layers.append(layer)\n",
    "            print(layer)\n",
    "        \n",
    "        self.logic_layers = nn.Sequential(*layers)\n",
    "        self.group = GroupSum(k=output_size, tau=tau)\n",
    "        self.log_text = \"\"  # initializes logging string\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the DiffLogic model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after processing through the logic layers and grouping operation.\n",
    "        \"\"\"\n",
    "        # moves tensor to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to('cuda')          \n",
    "        x = self.flatten(x)\n",
    "        logits = self.logic_layers(x)\n",
    "        group = self.group(logits)\n",
    "        return group\n",
    "    \n",
    "    def save(self, file_path, model_name='model'):\n",
    "        \"\"\"\n",
    "        Saves the model's state dictionary to the specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path where the model will be saved.\n",
    "            model_name (str): Name of the saved model\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'connections': [layer.indices for layer in self.logic_layers if isinstance(layer, LogicLayer)]\n",
    "        }, os.path.join(file_path, f\"{model_name}.pth\"))\n",
    "        self.log_text += f\"Model saved to: {file_path}\\n\"\n",
    "\n",
    "    def load(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads the model's state dictionary from the specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path from which the model will be loaded.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(file_path)\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # assigns connections to each LogicLayer\n",
    "        for idx, layer in enumerate(self.logic_layers):\n",
    "            if isinstance(layer, LogicLayer):\n",
    "                layer.indices = checkpoint['connections'][idx]\n",
    "\n",
    "        self.eval()\n",
    "        self.log_text += f\"Model loaded from: {file_path}\\n\"\n",
    "        \n",
    "    def get_accuracy(self, data_loader):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy of the model against a data loader\n",
    "\n",
    "        Args:\n",
    "            data_loader: a DataLoader object, e.g. train_loader or test_loader\n",
    "\n",
    "        Returns:\n",
    "            float: The accuracy\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # ensures that model is in evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            for batch_inputs, batch_outputs in tqdm(data_loader, desc=\"Running Inference\"):\n",
    "                batch_inputs, batch_outputs = batch_inputs.to('cuda'), batch_outputs.to('cuda')\n",
    "\n",
    "                # forward pass to get predictions\n",
    "                outputs = self(batch_inputs)\n",
    "\n",
    "                # gets the predicted class (index of the maximum logit)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # counting correct predictions\n",
    "                total += batch_outputs.size(0)  # total number of samples in the batch\n",
    "                correct += (predicted == batch_outputs).sum().item()  # counting correct predictions\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return accuracy\n",
    "\n",
    "    def get_log(self):\n",
    "        \"\"\"\n",
    "        Retrieves the log text and clears the log after retrieval.\n",
    "\n",
    "        Returns:\n",
    "            str: The log text.\n",
    "        \"\"\"\n",
    "        log_copy = self.log_text\n",
    "        self.log_text = \"\"  # Clear the log after returning\n",
    "        return log_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5632fa27-5e05-41b0-b3a3-ac941714657b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        \"\"\"\n",
    "        Initializes the EarlyStopper to stop training if the performance doesn't improve after a certain number of epochs.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait for an improvement.\n",
    "            min_delta (float): Minimum change to consider an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def should_stop(self, current_loss):\n",
    "        \"\"\"\n",
    "        Check if training should stop based on the current loss.\n",
    "\n",
    "        Args:\n",
    "            current_loss (float): The current loss.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if training should stop, False otherwise.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = current_loss\n",
    "            return False\n",
    "        elif current_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = current_loss\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(\"EarlyStopper Triggered: \", self.counter)\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd97e0b-c6c4-4262-97ab-32136126ba39",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dea90015-6e45-49af-bb11-539c8811adcd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model model_001\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:05<00:00, 36.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.32074836427403\n",
      "training model model_002\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:05<00:00, 36.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.8917434146691687\n",
      "training model model_003\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:05<00:00, 36.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.7277942065320826\n",
      "training model model_004\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:05<00:00, 36.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.6148660640140711\n",
      "training model model_005\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:05<00:00, 36.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.5767550575285024\n",
      "training model model_006\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:05<00:00, 35.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.105385702543363\n",
      "training model model_007\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 35.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.7779940420947878\n",
      "training model model_008\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 34.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.6840911213075435\n",
      "training model model_009\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 34.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.6394098691298195\n",
      "training model model_010\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 34.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.6131636172960194\n",
      "training model model_011\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 34.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.1009490500387262\n",
      "training model model_012\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 33.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.8509318877125692\n",
      "training model model_013\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 33.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.7775673166893113\n",
      "training model model_014\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 32.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.7164329647736766\n",
      "training model model_015\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 32.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.7021548332659013\n",
      "training model model_016\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 32.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.2559170365305472\n",
      "training model model_017\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 32.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.9839780515730577\n",
      "training model model_018\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 31.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.9308833051440867\n",
      "training model model_019\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 31.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.890469969632707\n",
      "training model model_020\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 30.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.8836843639523104\n",
      "training model model_021\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 32.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.5505694020463643\n",
      "training model model_022\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 30.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.3006506453786988\n",
      "training model model_023\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 30.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.2434800393998353\n",
      "training model model_024\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:07<00:00, 27.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.2033524443686654\n",
      "training model model_025\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:07<00:00, 26.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.1428047967563768\n",
      "All models processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize Hydra with the config path and job name\n",
    "with initialize(version_base=None, config_path=\"config\", job_name=\"aidays2024\"):\n",
    "    cfg = compose(config_name=\"mnist_config_16x16\")\n",
    "\n",
    "# training loop for all models\n",
    "all_models_dict = {}\n",
    "num_epochs = 1\n",
    "file_path = 'trained_models/mnist_trained_16x16' # where to save your trained models\n",
    "\n",
    "# loops through all model configs and trains each of them\n",
    "for model_name, model_cfg in cfg.models.items():\n",
    "    print(f'training model {model_name}')\n",
    "\n",
    "    # tracking dictionary\n",
    "    all_models_dict[model_name] = {\n",
    "        'losses': [],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # initializes DiffLogic model and moves to CUDA if available\n",
    "        model = DiffLogic(layers_config=model_cfg['layers_config'], \n",
    "                          output_size=model_cfg['output_size'], \n",
    "                          tau=model_cfg['tau']).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # optimizer and loss criterion\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=model_cfg['learning_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # early stopping\n",
    "        early_stopper = EarlyStopper(patience=5)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            loop = tqdm(train_loader, leave=True, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "            epoch_loss = 0  # to track loss for an epoch\n",
    "            \n",
    "            for batch_inputs, batch_outputs in loop:\n",
    "                # move data to the appropriate device\n",
    "                device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "                batch_inputs, batch_outputs = batch_inputs.to(device).double(), batch_outputs.to(device).long()\n",
    "\n",
    "                # forward pass through the model\n",
    "                predictions = model(batch_inputs)\n",
    "                loss = criterion(predictions, batch_outputs)\n",
    "\n",
    "                # zero gradients, backpropagates, and updates model parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # accumulating the loss for the epoch\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # caclulating the average loss for the epoch\n",
    "            epoch_loss /= len(train_loader)\n",
    "            all_models_dict[model_name]['losses'].append(epoch_loss)\n",
    "            print(f'Epoch {epoch+1} Loss: {epoch_loss}')\n",
    "\n",
    "            # checks for early stopping\n",
    "            if early_stopper.should_stop(epoch_loss):\n",
    "                print(f\"Early stopping triggered for {model_name} at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "        # saving trained model's state\n",
    "        model.save(file_path, model_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR TRAINING {model_name.upper()}: {str(e)}\")\n",
    "\n",
    "print(\"All models processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28a834-adac-4241-83a4-d9a23ac17db5",
   "metadata": {},
   "source": [
    "#### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33048f62-fc21-4f87-b725-6eb470a66700",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "Evaluating model_001.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:01<00:00, 33.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_001.pth: 81.55%\n",
      "\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "Evaluating model_002.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 38.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_002.pth: 86.65%\n",
      "\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "Evaluating model_003.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 38.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_003.pth: 88.55%\n",
      "\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "Evaluating model_004.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 38.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_004.pth: 89.41%\n",
      "\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "Evaluating model_005.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_005.pth: 90.03%\n",
      "\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "Evaluating model_006.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_006.pth: 84.95%\n",
      "\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "Evaluating model_007.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 38.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_007.pth: 89.87%\n",
      "\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "Evaluating model_008.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 38.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_008.pth: 91.46%\n",
      "\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "Evaluating model_009.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 36.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_009.pth: 92.41%\n",
      "\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "Evaluating model_010.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_010.pth: 93.10%\n",
      "\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "Evaluating model_011.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 38.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_011.pth: 86.13%\n",
      "\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "Evaluating model_012.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 38.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_012.pth: 89.95%\n",
      "\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "Evaluating model_013.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_013.pth: 91.75%\n",
      "\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "Evaluating model_014.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_014.pth: 92.70%\n",
      "\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "Evaluating model_015.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_015.pth: 93.83%\n",
      "\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "Evaluating model_016.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_016.pth: 84.79%\n",
      "\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "Evaluating model_017.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_017.pth: 89.77%\n",
      "\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "Evaluating model_018.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 36.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_018.pth: 91.35%\n",
      "\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "Evaluating model_019.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_019.pth: 92.42%\n",
      "\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "Evaluating model_020.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_020.pth: 93.13%\n",
      "\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "Evaluating model_021.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_021.pth: 81.64%\n",
      "\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "Evaluating model_022.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 36.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_022.pth: 86.44%\n",
      "\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "Evaluating model_023.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_023.pth: 88.89%\n",
      "\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "Evaluating model_024.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_024.pth: 90.10%\n",
      "\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "Evaluating model_025.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 36.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_025.pth: 90.42%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# testing loop to test inferences\n",
    "trained_models_dir = 'trained_models/mnist_trained_16x16'\n",
    "\n",
    "# retrieves a list of all model files in the directory\n",
    "model_files = sorted([f for f in os.listdir(trained_models_dir) if f.endswith('.pth')])\n",
    "\n",
    "with initialize(version_base=None, config_path=\"config\", job_name=\"test_app\"):\n",
    "    cfg = compose(config_name=\"mnist_config_16x16\")\n",
    "\n",
    "# dictionary to store the trained models\n",
    "trained_models = {}\n",
    "\n",
    "# loops through all model files and calculates their accuracies\n",
    "for i, model_file in enumerate(model_files):\n",
    "    if model_file.endswith('_weights.pth'):\n",
    "        model_name = model_file.removesuffix('_weights.pth')\n",
    "    else:\n",
    "        model_name = model_file.removesuffix('.pth')\n",
    "    \n",
    "    model_cfg = cfg['models'][model_name]\n",
    "    \n",
    "    # instantiates the model and load its weights\n",
    "    model = DiffLogic(layers_config=model_cfg['layers_config'], \n",
    "                          output_size=model_cfg['output_size'], \n",
    "                          tau=model_cfg['tau']).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_path = os.path.join(trained_models_dir, model_file)\n",
    "    print(f\"Evaluating {model_file}...\")\n",
    "\n",
    "    # loads the respective model\n",
    "    model.load(model_path)\n",
    "\n",
    "    # calculates accuracy\n",
    "    accuracy = model.get_accuracy(test_loader)\n",
    "    \n",
    "    print(f\"Accuracy of {model_file}: {accuracy * 100:.2f}%\\n\")\n",
    "    \n",
    "    trained_models[i] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd2b5d-ed32-4ff0-9c40-caa7169ae1ed",
   "metadata": {},
   "source": [
    "#### Model Optimization (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8665f3c-8c20-400e-a147-d92abac655eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# maybe remove the unused nodes to increase inference speed / decrease energy consumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c06262-afe8-4b02-883c-c044a060fc39",
   "metadata": {},
   "source": [
    "#### Verilog Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e20496-c3ac-4d87-8e55-359ac0069b03",
   "metadata": {},
   "source": [
    "Logic gate to Verilog expression mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2846e8b8-9b6f-49dc-adbc-7bfe8fe0a27d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logic_gate_verilog = {\n",
    "    \"0\": \"1'b0\",\n",
    "    \"A∧B\": \"({a}) & ({b})\",\n",
    "    \"¬(A⇒B)\": \"({a}) & ~({b})\",\n",
    "    \"A\": \"{a}\",\n",
    "    \"¬(B⇒A)\": \"({b}) & ~({a})\",\n",
    "    \"B\": \"{b}\",\n",
    "    \"A⊕B\": \"({a}) ^ ({b})\",\n",
    "    \"A∨B\": \"({a}) | ({b})\",\n",
    "    \"¬(A∨B)\": \"~(({a}) | ({b}))\",\n",
    "    \"¬(A⊕B)\": \"~(({a}) ^ ({b}))\",\n",
    "    \"¬B\": \"~({b})\",\n",
    "    \"B⇒A\": \"~({b}) | ({a})\",\n",
    "    \"¬A\": \"~({a})\",\n",
    "    \"A⇒B\": \"~({a}) | ({b})\",\n",
    "    \"¬(A∧B)\": \"~(({a}) & ({b}))\",\n",
    "    \"1\": \"1'b1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0a28141-16b1-4e15-8adc-a4aa7cb405cf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "side = 16 # pixels in one side of the image\n",
    "N_input = 16 * 16  # number of input dimensions\n",
    "N_output = 10  # number of output dimensions (classes)\n",
    "\n",
    "# converts the learned logic gates to verilog or vhdl \n",
    "def generate_verilog(model, filename=\"logic_network.v\"):\n",
    "    \n",
    "    N_layers = len(model.logic_layers)\n",
    "\n",
    "    # gets number of neurons per layer\n",
    "    neurons_per_layer = [layer.weights.size()[0] for layer in model.logic_layers]\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        # module declaration\n",
    "        file.write(\"module logic_network(\\n\")\n",
    "        file.write(f\"    input wire [{N_input-1}:0] inputs,\\n\")\n",
    "        file.write(f\"    output wire [{N_output-1}:0] outputs\\n\")\n",
    "        file.write(\");\\n\\n\")\n",
    "\n",
    "        # declares wires for internal layers\n",
    "        for layer_index in range(N_layers - 1):\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "            file.write(f\"    wire [{N_neurons -1}:0] layer{layer_index}_outputs;\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "        logic_operations = list(logic_gate_verilog.keys())\n",
    "\n",
    "        for layer_index in range(N_layers):\n",
    "            logic_layer = model.logic_layers[layer_index]\n",
    "\n",
    "            # gets input and output indices\n",
    "            input_indices = logic_layer.indices[0].cpu().numpy()  # first input indices\n",
    "            output_indices = logic_layer.indices[1].cpu().numpy()  # second input indices\n",
    "\n",
    "            neuron_gates = [torch.argmax(logic_layer.weights[neuron]).item()\n",
    "                            for neuron in range(logic_layer.weights.size()[0])]\n",
    "            connections = {i: (input_indices[i], output_indices[i]) for i in range(len(neuron_gates))}\n",
    "\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "\n",
    "            # determines input wires\n",
    "            if layer_index == 0:\n",
    "                input_wire_base = \"inputs\"\n",
    "            else:\n",
    "                input_wire_base = f\"layer{layer_index -1}_outputs\"\n",
    "\n",
    "            # determines output wires\n",
    "            if layer_index == N_layers - 1:\n",
    "                output_wire_base = \"outputs\"\n",
    "            else:\n",
    "                output_wire_base = f\"layer{layer_index}_outputs\"\n",
    "\n",
    "            # assign statements for this layer\n",
    "            for neuron_id in range(N_neurons):\n",
    "                a_idx, b_idx = connections[neuron_id]\n",
    "\n",
    "                # maps indices to input wires\n",
    "                a_wire = f\"{input_wire_base}[{a_idx}]\"\n",
    "                b_wire = f\"{input_wire_base}[{b_idx}]\"\n",
    "\n",
    "                # gets gate\n",
    "                gate_op = logic_operations[neuron_gates[neuron_id]]\n",
    "                gate = logic_gate_verilog[gate_op].format(a=a_wire, b=b_wire)\n",
    "\n",
    "                # assigns to output wire\n",
    "                output_wire = f\"{output_wire_base}[{neuron_id}]\"\n",
    "\n",
    "                file.write(f\"    assign {output_wire} = {gate};\\n\")\n",
    "\n",
    "        file.write(\"endmodule\\n\")\n",
    "        print('success')\n",
    "\n",
    "# generates Verilog file for all trained models\n",
    "for model_idx in range(len(trained_models)):\n",
    "    i = model_idx + 1\n",
    "    generate_verilog(trained_models[model_idx], filename=f\"verilog/{side}x{side}/model_{i:03d}_logic_network.v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7325be-1e5f-4c16-8a49-a66327d183bd",
   "metadata": {},
   "source": [
    "#### VHDL Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22f5f2-059b-453e-b32e-9634cf8a9e3c",
   "metadata": {},
   "source": [
    "Logic gate to Verilog expression mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78b2f39a-e420-4b4d-93a6-1c0182bd15b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logic_gate_vhdl = {\n",
    "    \"0\": \"'0'\",\n",
    "    \"A∧B\": \"({a}) and ({b})\",\n",
    "    \"¬(A⇒B)\": \"({a}) and not ({b})\",\n",
    "    \"A\": \"{a}\",\n",
    "    \"¬(B⇒A)\": \"({b}) and not ({a})\",\n",
    "    \"B\": \"{b}\",\n",
    "    \"A⊕B\": \"({a}) xor ({b})\",\n",
    "    \"A∨B\": \"({a}) or ({b})\",\n",
    "    \"¬(A∨B)\": \"not(({a}) or ({b}))\",\n",
    "    \"¬(A⊕B)\": \"not(({a}) xor ({b}))\",\n",
    "    \"¬B\": \"not({b})\",\n",
    "    \"B⇒A\": \"not({b}) or ({a})\",\n",
    "    \"¬A\": \"not({a})\",\n",
    "    \"A⇒B\": \"not({a}) or ({b})\",\n",
    "    \"¬(A∧B)\": \"not(({a}) and ({b}))\",\n",
    "    \"1\": \"'1'\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20389672-5297-4dd4-a368-1b96df1445ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "side = 16 # pixels in one side of the image\n",
    "N_input = 16 * 16  # number of input dimensions\n",
    "N_output = 10  # number of output dimensions (classes)\n",
    "\n",
    "# Converts the learned logic gates to VHDL\n",
    "def generate_vhdl(model, filename=\"logic_network.vhdl\"):\n",
    "    N_layers = len(model.logic_layers)\n",
    "    neurons_per_layer = [layer.weights.size()[0] for layer in model.logic_layers]\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        # Library and entity declaration\n",
    "        file.write(\"library IEEE;\\n\")\n",
    "        file.write(\"use IEEE.STD_LOGIC_1164.ALL;\\n\\n\")\n",
    "        file.write(\"entity logic_network is\\n\")\n",
    "        file.write(f\"    port (\\n\")\n",
    "        file.write(f\"        inputs : in std_logic_vector({N_input - 1} downto 0);\\n\")\n",
    "        file.write(f\"        outputs : out std_logic_vector({N_output - 1} downto 0)\\n\")\n",
    "        file.write(\"    );\\n\")\n",
    "        file.write(\"end logic_network;\\n\\n\")\n",
    "        file.write(\"architecture Behavioral of logic_network is\\n\")\n",
    "\n",
    "        # Declare signals for internal layers\n",
    "        for layer_index in range(N_layers - 1):\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "            file.write(f\"    signal layer{layer_index}_outputs : std_logic_vector({N_neurons - 1} downto 0);\\n\")\n",
    "        file.write(\"\\nbegin\\n\\n\")\n",
    "\n",
    "        logic_operations = list(logic_gate_vhdl.keys())\n",
    "\n",
    "        # Generate VHDL code for each layer\n",
    "        for layer_index in range(N_layers):\n",
    "            logic_layer = model.logic_layers[layer_index]\n",
    "\n",
    "            # Get input and output indices\n",
    "            input_indices = logic_layer.indices[0].cpu().numpy()  # first input indices\n",
    "            output_indices = logic_layer.indices[1].cpu().numpy()  # second input indices\n",
    "\n",
    "            neuron_gates = [torch.argmax(logic_layer.weights[neuron]).item()\n",
    "                            for neuron in range(logic_layer.weights.size()[0])]\n",
    "            connections = {i: (input_indices[i], output_indices[i]) for i in range(len(neuron_gates))}\n",
    "\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "\n",
    "            # Determine input signals\n",
    "            if layer_index == 0:\n",
    "                input_wire_base = \"inputs\"\n",
    "            else:\n",
    "                input_wire_base = f\"layer{layer_index -1}_outputs\"\n",
    "\n",
    "            # Determine output signals\n",
    "            if layer_index == N_layers - 1:\n",
    "                output_wire_base = \"outputs\"\n",
    "            else:\n",
    "                output_wire_base = f\"layer{layer_index}_outputs\"\n",
    "\n",
    "            # Assign statements for each neuron in this layer\n",
    "            for neuron_id in range(N_neurons):\n",
    "                a_idx, b_idx = connections[neuron_id]\n",
    "\n",
    "                # Map indices to input signals\n",
    "                a_wire = f\"{input_wire_base}({a_idx})\"\n",
    "                b_wire = f\"{input_wire_base}({b_idx})\"\n",
    "\n",
    "                # Get the gate operation\n",
    "                gate_op = logic_operations[neuron_gates[neuron_id]]\n",
    "                gate = logic_gate_vhdl[gate_op].format(a=a_wire, b=b_wire)\n",
    "\n",
    "                # Assign to output signal\n",
    "                output_wire = f\"{output_wire_base}({neuron_id})\"\n",
    "                file.write(f\"    {output_wire} <= {gate};\\n\")\n",
    "\n",
    "        file.write(\"\\nend Behavioral;\\n\")\n",
    "        print('success')\n",
    "\n",
    "# Generates VHDL files for all trained models\n",
    "for model_idx in range(len(trained_models)):\n",
    "    i = model_idx + 1\n",
    "    generate_vhdl(trained_models[model_idx], filename=f\"vhdl/{side}x{side}/model_{i:03d}_logic_network.vhdl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d3597-9cf6-4688-a0ab-a676848f4439",
   "metadata": {},
   "source": [
    "#### Predicting from Hex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59df202-6740-4136-82ad-2ad266c20eb6",
   "metadata": {},
   "source": [
    "Sanity check to see that the FPGA output matches the model predictions on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dac4a4f5-4b7f-47be-bf77-6864016839c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid line: 102 : 1.8001800180018E+62;\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def load_images_from_hex_file(filename):\n",
    "    images = []\n",
    "    hex_pattern = re.compile(r'^[0-9a-fA-F]+$')  # only allows valid hex characters\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if ':' in line:\n",
    "                hex_data = line.split(':')[1].strip().strip(';')\n",
    "                \n",
    "                # check if hex_data is a valid hexadecimal string\n",
    "                if hex_pattern.match(hex_data):\n",
    "                    # convert to binary and pad to 256 bits\n",
    "                    bin_data = bin(int(hex_data, 16))[2:].zfill(256)\n",
    "                    image = np.array([int(bit) for bit in bin_data], dtype=np.uint8).reshape(16, 16)\n",
    "                    images.append(image)\n",
    "                else:\n",
    "                    print(f\"Skipping invalid line: {line.strip()}\")\n",
    "\n",
    "    return np.array(images)\n",
    "\n",
    "# loading images\n",
    "filename = 'mnist_input.mif'  \n",
    "images = load_images_from_hex_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a62a3e-3426-4430-a11e-3f49cf0a582a",
   "metadata": {},
   "source": [
    "Displays a random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ec3f7637-d060-4831-a894-7ceed63368d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = random.randint(0, 1024)\n",
    "\n",
    "if idx == 102: # line that is broken\n",
    "    idx = 0\n",
    "\n",
    "# plot the image\n",
    "plt.imshow(images[idx], cmap='gray')\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fe7da784-e8c6-47b5-af6f-4c8b09e1dcbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model4 = trained_models[0]\n",
    "\n",
    "def run_inference_on_images(model, images):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    \n",
    "    for img in images:\n",
    "        # flatten each 16x16 image to a 1D tensor with 256 elements\n",
    "        input_data = torch.tensor(img.flatten(), dtype=torch.float32).unsqueeze(0).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        with torch.no_grad():  # disable gradient computation for inference\n",
    "            output = model(input_data)\n",
    "            _, predicted_label = torch.max(output, 1)  # get the predicted label\n",
    "            predictions.append(predicted_label.item())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "predictions = run_inference_on_images(trained_models[24], images)\n",
    "\n",
    "# displays some sample images with predictions\n",
    "for i in range(5):  # Display first 5 images as examples\n",
    "    plt.imshow(images[i], cmap='gray')\n",
    "    plt.title(f\"Predicted Label: {predictions[i]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIFFLOGIC",
   "language": "python",
   "name": "difflogic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
