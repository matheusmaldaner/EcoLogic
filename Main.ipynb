{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df65bb0b-594d-4717-bd3c-63b04b9d864b",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb5dd1c0-73cf-4679-ae21-6fa00a85a395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import yaml\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Compose, Lambda\n",
    "import mnist_dataset\n",
    "from hydra import initialize, compose\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, Subset\n",
    "from difflogic import LogicLayer, GroupSum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99c41f-b5c7-4d64-a704-5a003a5b56c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8096d-fc17-46d7-861e-685d2fe12dbb",
   "metadata": {},
   "source": [
    "Tunable Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47ea8409-a084-4bee-8121-96acd99bb28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# True: Removes border of Mnist, False: Keeps black border around digits\n",
    "remove_border = False\n",
    "# True: Binarized Images, False: Grayscale Images \n",
    "binarize_images = True  \n",
    "# True: Even distribution of samples, False: Original Mnist distribution \n",
    "evenly_partitioned = True\n",
    "# True: Upscales the samples to 32x32, False: Keeps size unchanged\n",
    "upscaled_images = False\n",
    "# True: Downscales the samples to 16x16, False: Keeps size unchanged\n",
    "downscaled_images = True\n",
    "# Batch size\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4450630-e7a8-4564-b106-6e2dc3a12e61",
   "metadata": {},
   "source": [
    "Dataset Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1779afcd-2def-41d1-abdc-c1bc8dbb1719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to binarize an image, threshold is tunable \n",
    "def binarize(image, threshold=0.5):\n",
    "    return (image > threshold).float()  \n",
    "\n",
    "# define the transformation logic based on the toggle\n",
    "transform_list = [ToTensor()]\n",
    "if upscaled_images:\n",
    "    transform_list.append(Resize((32, 32)))\n",
    "elif downscaled_images:\n",
    "    transform_list.append(Resize((16, 16)))\n",
    "if binarize_images:\n",
    "    transform_list.append(Lambda(lambda x: binarize(x)))\n",
    "    \n",
    "transform = Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d9255d2-bf0c-4bab-b66a-b65360bcb86f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = mnist_dataset.MNIST('./data-mnist', train=True, download=True, remove_border=remove_border, transform=transform)\n",
    "test_dataset = mnist_dataset.MNIST('./data-mnist', train=False, remove_border=remove_border, transform=transform)\n",
    "\n",
    "# drop_last = True means it will drop the last incomplete Batch\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38a5db70-d2ba-44a7-803c-829177a182c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes the Dataset evenly partitioned\n",
    "if evenly_partitioned:\n",
    "    # code below is used so that all classes have the same number of samples\n",
    "    train_targets = train_loader.dataset.targets\n",
    "    test_targets = test_loader.dataset.targets\n",
    "\n",
    "    train_digits_total = []\n",
    "    test_digits_total = []\n",
    "\n",
    "    for i in range(10):\n",
    "        curr_tot_train = torch.sum(train_targets == i).item()\n",
    "        curr_tot_test = torch.sum(test_targets == i).item()    \n",
    "        train_digits_total.append(curr_tot_train)\n",
    "        test_digits_total.append(curr_tot_test)\n",
    "\n",
    "    train_digits_total, test_digits_total\n",
    "\n",
    "    # find the minimum number of samples across all classes\n",
    "    min_samples_train = min(train_digits_total)\n",
    "    min_samples_test = min(test_digits_total)\n",
    "\n",
    "    # function to trim dataset to match the minimum samples for each class and shuffle indices\n",
    "    def trim_dataset(dataset, targets, min_samples):\n",
    "        indices = []\n",
    "        for i in range(10):\n",
    "            class_indices = (targets == i).nonzero(as_tuple=True)[0]  # get indices of class i\n",
    "            class_indices = class_indices[:min_samples]  # trim to min_samples\n",
    "            indices.extend(class_indices)\n",
    "\n",
    "        # shuffle indices after collecting them\n",
    "        indices = torch.tensor(indices)\n",
    "        indices = indices[torch.randperm(indices.size(0))]  \n",
    "\n",
    "        return Subset(dataset, indices)\n",
    "\n",
    "    # trim both train and test datasets to ensure all classes have the same number of samples\n",
    "    trimmed_train_dataset = trim_dataset(train_loader.dataset, train_targets, min_samples_train)\n",
    "    trimmed_test_dataset = trim_dataset(test_loader.dataset, test_targets, min_samples_test)\n",
    "\n",
    "    # create DataLoaders for the trimmed datasets\n",
    "    trimmed_train_loader = DataLoader(trimmed_train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    trimmed_test_loader = DataLoader(trimmed_test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "\n",
    "    # verify the lengths of the trimmed datasets\n",
    "    len(trimmed_train_loader.dataset), len(trimmed_test_loader.dataset)\n",
    "\n",
    "    train_dataset = trimmed_train_dataset\n",
    "    test_dataset = trimmed_test_dataset\n",
    "    train_loader = trimmed_train_loader\n",
    "    test_loader = trimmed_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aebb9b7f-2399-4b9f-8a8a-570c922f92ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54210, 8920)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = len(train_loader.dataset) + len(test_loader.dataset)\n",
    "len(train_loader.dataset), len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66230413-408e-4bc1-bf5f-2313690e0964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14c421dff760>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_size = len(train_dataset)\n",
    "random_index = random.randint(0, dataset_size - 1)\n",
    "\n",
    "if remove_border:\n",
    "    image = train_loader.dataset[random_index][0].reshape(20, 20)\n",
    "elif not remove_border and not upscaled_images and not downscaled_images:\n",
    "    image = train_loader.dataset[random_index][0].reshape(28, 28)\n",
    "elif downscaled_images:\n",
    "    image = train_loader.dataset[random_index][0].reshape(16, 16)\n",
    "else:\n",
    "    image = train_loader.dataset[random_index][0].reshape(32, 32)\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973f0e3-5f32-4793-afc6-9394425419de",
   "metadata": {},
   "source": [
    "#### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e63be-c489-4aaf-a605-9e1d0fda631f",
   "metadata": {},
   "source": [
    "Converts csv into yaml config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9b89fd2-f200-40c1-973a-489ef4190e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define first input and the name of the file to be saved\n",
    "first_in_dim = 256 # 16x16\n",
    "filename = \"config/mnist_config_16x16.yaml\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dc64546-f1d7-4e18-b88c-7140d4d42692",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML file 'config/mnist_config_16x16.yaml' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# reads the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"config/hyperparameters.csv\")\n",
    "\n",
    "# convert the DataFrame to a list of dictionaries\n",
    "models = df.to_dict(orient=\"records\")\n",
    "\n",
    "# create the YAML structure\n",
    "yaml_structure = {\"models\": {}}\n",
    "\n",
    "# rounds the number to the nearest multiple of the output size\n",
    "def round_to_nearest_multiple(value, multiple):\n",
    "    return multiple * round(value / multiple)\n",
    "\n",
    "# populate the YAML structure with models\n",
    "for i, model in enumerate(models, start=1):\n",
    "    # zero-padding model names to 3 digits \n",
    "    model_name = f\"model_{str(i).zfill(3)}\"\n",
    "    layers_config = {}\n",
    "    \n",
    "    for layer in range(1, model[\"H\"] + 1):\n",
    "        # zero-padding the layer names to 3 digits\n",
    "        layer_name = f\"LogicLayer{str(layer).zfill(3)}\"\n",
    "        \n",
    "        # adjusts in_dim to the nearest multiple of 10\n",
    "        in_dim = first_in_dim if layer == 1 else round_to_nearest_multiple(model[\"W\"], 10)\n",
    "        \n",
    "        # adjusts out_dim to the nearest multiple of 10\n",
    "        out_dim = round_to_nearest_multiple(model[\"W\"], 10)\n",
    "        \n",
    "        layers_config[layer_name] = {\n",
    "            \"in_dim\": in_dim,\n",
    "            \"out_dim\": out_dim,\n",
    "            \"device\": \"cuda\",\n",
    "            \"implementation\": \"cuda\",\n",
    "            \"connections\": \"random\",\n",
    "            \"grad_factor\": 2, # we can try different grad_factor values as well\n",
    "        }\n",
    "    \n",
    "    yaml_structure[\"models\"][model_name] = {\n",
    "        \"input_dim\": first_in_dim, \n",
    "        \"output_size\": 10, # for MNIST classification\n",
    "        \"tau\": model[\"tau\"],\n",
    "        \"learning_rate\": model[\"lr\"],\n",
    "        \"layers_config\": layers_config,\n",
    "    }\n",
    "\n",
    "# saves to a YAML file\n",
    "with open(f'{filename}', \"w\") as file:\n",
    "    yaml.dump(yaml_structure, file, default_flow_style=False)\n",
    "\n",
    "print(f\"YAML file '{filename}' generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e939ace-e4c3-4781-bdea-f9722ddb925e",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473370c-ec74-455b-854c-f597d04144f3",
   "metadata": {},
   "source": [
    "DiffLogic Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b244edec-9bd5-474d-9aaa-4f9aafe31b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffLogic(nn.Module):\n",
    "    def __init__(self, layers_config, output_size, tau=30):\n",
    "        \"\"\"\n",
    "        Initializes the DiffLogic model with the specified layer configurations, output size, and temperature parameter.\n",
    "\n",
    "        Args:\n",
    "            layers_config (dict): Configuration for each logic layer, including dimensions, device, implementation, connections, and grad factor.\n",
    "            output_size (int): The number of output groups (classes in a classification problem).\n",
    "            tau (int): Temperature parameter for the GroupSum operation.\n",
    "        \"\"\"\n",
    "        super(DiffLogic, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # stores the logic layers\n",
    "        layers = []\n",
    "        for layer_name, config in layers_config.items():\n",
    "            layer = LogicLayer(\n",
    "                in_dim=config['in_dim'],\n",
    "                out_dim=config['out_dim'],\n",
    "                device=config['device'],\n",
    "                implementation=config['implementation'],\n",
    "                connections=config['connections'],\n",
    "                grad_factor=config['grad_factor']       \n",
    "            )\n",
    "            layers.append(layer)\n",
    "            print(layer)\n",
    "        \n",
    "        self.logic_layers = nn.Sequential(*layers)\n",
    "        self.group = GroupSum(k=output_size, tau=tau)\n",
    "        self.log_text = \"\"  # initializes logging string\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the DiffLogic model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after processing through the logic layers and grouping operation.\n",
    "        \"\"\"\n",
    "        # moves tensor to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to('cuda')          \n",
    "        x = self.flatten(x)\n",
    "        logits = self.logic_layers(x)\n",
    "        group = self.group(logits)\n",
    "        return group\n",
    "    \n",
    "    def save(self, file_path, model_name='model'):\n",
    "        \"\"\"\n",
    "        Saves the model's state dictionary to the specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path where the model will be saved.\n",
    "            model_name (str): Name of the saved model\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'connections': [layer.indices for layer in self.logic_layers if isinstance(layer, LogicLayer)]\n",
    "        }, os.path.join(file_path, f\"{model_name}.pth\"))\n",
    "        self.log_text += f\"Model saved to: {file_path}\\n\"\n",
    "\n",
    "    def load(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads the model's state dictionary from the specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path from which the model will be loaded.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(file_path)\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # assigns connections to each LogicLayer\n",
    "        for idx, layer in enumerate(self.logic_layers):\n",
    "            if isinstance(layer, LogicLayer):\n",
    "                layer.indices = checkpoint['connections'][idx]\n",
    "\n",
    "        self.eval()\n",
    "        self.log_text += f\"Model loaded from: {file_path}\\n\"\n",
    "        \n",
    "    def get_accuracy(self, data_loader):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy of the model against a data loader\n",
    "\n",
    "        Args:\n",
    "            data_loader: a DataLoader object, e.g. train_loader or test_loader\n",
    "\n",
    "        Returns:\n",
    "            float: The accuracy\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # ensures that model is in evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            for batch_inputs, batch_outputs in tqdm(data_loader, desc=\"Running Inference\"):\n",
    "                batch_inputs, batch_outputs = batch_inputs.to('cuda'), batch_outputs.to('cuda')\n",
    "\n",
    "                # forward pass to get predictions\n",
    "                outputs = self(batch_inputs)\n",
    "\n",
    "                # gets the predicted class (index of the maximum logit)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # counting correct predictions\n",
    "                total += batch_outputs.size(0)  # total number of samples in the batch\n",
    "                correct += (predicted == batch_outputs).sum().item()  # counting correct predictions\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return accuracy\n",
    "\n",
    "    def get_log(self):\n",
    "        \"\"\"\n",
    "        Retrieves the log text and clears the log after retrieval.\n",
    "\n",
    "        Returns:\n",
    "            str: The log text.\n",
    "        \"\"\"\n",
    "        log_copy = self.log_text\n",
    "        self.log_text = \"\"  # Clear the log after returning\n",
    "        return log_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5632fa27-5e05-41b0-b3a3-ac941714657b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        \"\"\"\n",
    "        Initializes the EarlyStopper to stop training if the performance doesn't improve after a certain number of epochs.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait for an improvement.\n",
    "            min_delta (float): Minimum change to consider an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def should_stop(self, current_loss):\n",
    "        \"\"\"\n",
    "        Check if training should stop based on the current loss.\n",
    "\n",
    "        Args:\n",
    "            current_loss (float): The current loss.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if training should stop, False otherwise.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = current_loss\n",
    "            return False\n",
    "        elif current_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = current_loss\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(\"EarlyStopper Triggered: \", self.counter)\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd97e0b-c6c4-4262-97ab-32136126ba39",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dea90015-6e45-49af-bb11-539c8811adcd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model model_001\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:05<00:00, 36.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.32074836427403\n",
      "training model model_002\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:05<00:00, 36.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.8917434146691687\n",
      "training model model_003\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:05<00:00, 36.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.7277942065320826\n",
      "training model model_004\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:05<00:00, 36.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.6148660640140711\n",
      "training model model_005\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:05<00:00, 36.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.5767550575285024\n",
      "training model model_006\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:05<00:00, 35.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.105385702543363\n",
      "training model model_007\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 35.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.7779940420947878\n",
      "training model model_008\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 34.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.6840911213075435\n",
      "training model model_009\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 34.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.6394098691298195\n",
      "training model model_010\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 34.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.6131636172960194\n",
      "training model model_011\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 34.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.1009490500387262\n",
      "training model model_012\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 33.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.8509318877125692\n",
      "training model model_013\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 33.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.7775673166893113\n",
      "training model model_014\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 32.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.7164329647736766\n",
      "training model model_015\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 32.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.7021548332659013\n",
      "training model model_016\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 32.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.2559170365305472\n",
      "training model model_017\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 32.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.9839780515730577\n",
      "training model model_018\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 31.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.9308833051440867\n",
      "training model model_019\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 31.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.890469969632707\n",
      "training model model_020\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 30.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.8836843639523104\n",
      "training model model_021\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 32.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.5505694020463643\n",
      "training model model_022\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 30.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.3006506453786988\n",
      "training model model_023\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:06<00:00, 30.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.2434800393998353\n",
      "training model model_024\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:07<00:00, 27.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.2033524443686654\n",
      "training model model_025\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 211/211 [00:07<00:00, 26.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.1428047967563768\n",
      "All models processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize Hydra with the config path and job name\n",
    "with initialize(version_base=None, config_path=\"config\", job_name=\"aidays2024\"):\n",
    "    cfg = compose(config_name=\"mnist_config_16x16\")\n",
    "\n",
    "# training loop for all models\n",
    "all_models_dict = {}\n",
    "num_epochs = 1\n",
    "file_path = 'trained_models/mnist_trained_16x16' # where to save your trained models\n",
    "\n",
    "# loops through all model configs and trains each of them\n",
    "for model_name, model_cfg in cfg.models.items():\n",
    "    print(f'training model {model_name}')\n",
    "\n",
    "    # tracking dictionary\n",
    "    all_models_dict[model_name] = {\n",
    "        'losses': [],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # initializes DiffLogic model and moves to CUDA if available\n",
    "        model = DiffLogic(layers_config=model_cfg['layers_config'], \n",
    "                          output_size=model_cfg['output_size'], \n",
    "                          tau=model_cfg['tau']).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # optimizer and loss criterion\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=model_cfg['learning_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # early stopping\n",
    "        early_stopper = EarlyStopper(patience=5)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            loop = tqdm(train_loader, leave=True, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "            epoch_loss = 0  # to track loss for an epoch\n",
    "            \n",
    "            for batch_inputs, batch_outputs in loop:\n",
    "                # move data to the appropriate device\n",
    "                device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "                batch_inputs, batch_outputs = batch_inputs.to(device).double(), batch_outputs.to(device).long()\n",
    "\n",
    "                # forward pass through the model\n",
    "                predictions = model(batch_inputs)\n",
    "                loss = criterion(predictions, batch_outputs)\n",
    "\n",
    "                # zero gradients, backpropagates, and updates model parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # accumulating the loss for the epoch\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # caclulating the average loss for the epoch\n",
    "            epoch_loss /= len(train_loader)\n",
    "            all_models_dict[model_name]['losses'].append(epoch_loss)\n",
    "            print(f'Epoch {epoch+1} Loss: {epoch_loss}')\n",
    "\n",
    "            # checks for early stopping\n",
    "            if early_stopper.should_stop(epoch_loss):\n",
    "                print(f\"Early stopping triggered for {model_name} at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "        # saving trained model's state\n",
    "        model.save(file_path, model_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR TRAINING {model_name.upper()}: {str(e)}\")\n",
    "\n",
    "print(\"All models processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28a834-adac-4241-83a4-d9a23ac17db5",
   "metadata": {},
   "source": [
    "#### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33048f62-fc21-4f87-b725-6eb470a66700",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "Evaluating model_001.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:01<00:00, 33.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_001.pth: 81.55%\n",
      "\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "Evaluating model_002.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 38.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_002.pth: 86.65%\n",
      "\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "Evaluating model_003.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 38.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_003.pth: 88.55%\n",
      "\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "Evaluating model_004.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 38.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_004.pth: 89.41%\n",
      "\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "Evaluating model_005.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_005.pth: 90.03%\n",
      "\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "Evaluating model_006.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_006.pth: 84.95%\n",
      "\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "Evaluating model_007.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 38.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_007.pth: 89.87%\n",
      "\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "Evaluating model_008.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 38.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_008.pth: 91.46%\n",
      "\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "Evaluating model_009.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 36.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_009.pth: 92.41%\n",
      "\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "Evaluating model_010.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_010.pth: 93.10%\n",
      "\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "Evaluating model_011.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 38.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_011.pth: 86.13%\n",
      "\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "Evaluating model_012.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 38.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_012.pth: 89.95%\n",
      "\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "Evaluating model_013.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_013.pth: 91.75%\n",
      "\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "Evaluating model_014.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_014.pth: 92.70%\n",
      "\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "Evaluating model_015.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_015.pth: 93.83%\n",
      "\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "Evaluating model_016.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_016.pth: 84.79%\n",
      "\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "Evaluating model_017.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_017.pth: 89.77%\n",
      "\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "Evaluating model_018.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 36.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_018.pth: 91.35%\n",
      "\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "Evaluating model_019.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_019.pth: 92.42%\n",
      "\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "Evaluating model_020.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_020.pth: 93.13%\n",
      "\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "Evaluating model_021.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_021.pth: 81.64%\n",
      "\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "Evaluating model_022.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 36.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_022.pth: 86.44%\n",
      "\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "Evaluating model_023.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_023.pth: 88.89%\n",
      "\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "Evaluating model_024.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 37.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_024.pth: 90.10%\n",
      "\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "Evaluating model_025.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:00<00:00, 36.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_025.pth: 90.42%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# testing loop to test inferences\n",
    "trained_models_dir = 'trained_models/mnist_trained_16x16'\n",
    "\n",
    "# retrieves a list of all model files in the directory\n",
    "model_files = sorted([f for f in os.listdir(trained_models_dir) if f.endswith('.pth')])\n",
    "\n",
    "with initialize(version_base=None, config_path=\"config\", job_name=\"test_app\"):\n",
    "    cfg = compose(config_name=\"mnist_config_16x16\")\n",
    "\n",
    "# dictionary to store the trained models\n",
    "trained_models = {}\n",
    "\n",
    "# loops through all model files and calculates their accuracies\n",
    "for i, model_file in enumerate(model_files):\n",
    "    if model_file.endswith('_weights.pth'):\n",
    "        model_name = model_file.removesuffix('_weights.pth')\n",
    "    else:\n",
    "        model_name = model_file.removesuffix('.pth')\n",
    "    \n",
    "    model_cfg = cfg['models'][model_name]\n",
    "    \n",
    "    # instantiates the model and load its weights\n",
    "    model = DiffLogic(layers_config=model_cfg['layers_config'], \n",
    "                          output_size=model_cfg['output_size'], \n",
    "                          tau=model_cfg['tau']).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_path = os.path.join(trained_models_dir, model_file)\n",
    "    print(f\"Evaluating {model_file}...\")\n",
    "\n",
    "    # loads the respective model\n",
    "    model.load(model_path)\n",
    "\n",
    "    # calculates accuracy\n",
    "    accuracy = model.get_accuracy(test_loader)\n",
    "    \n",
    "    print(f\"Accuracy of {model_file}: {accuracy * 100:.2f}%\\n\")\n",
    "    \n",
    "    trained_models[i] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd2b5d-ed32-4ff0-9c40-caa7169ae1ed",
   "metadata": {},
   "source": [
    "#### Model Optimization (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8665f3c-8c20-400e-a147-d92abac655eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# maybe remove the unused nodes to increase inference speed / decrease energy consumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c06262-afe8-4b02-883c-c044a060fc39",
   "metadata": {},
   "source": [
    "#### Verilog Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e20496-c3ac-4d87-8e55-359ac0069b03",
   "metadata": {},
   "source": [
    "Logic gate to Verilog expression mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2846e8b8-9b6f-49dc-adbc-7bfe8fe0a27d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logic_gate_verilog = {\n",
    "    \"0\": \"1'b0\",\n",
    "    \"A∧B\": \"({a}) & ({b})\",\n",
    "    \"¬(A⇒B)\": \"({a}) & ~({b})\",\n",
    "    \"A\": \"{a}\",\n",
    "    \"¬(B⇒A)\": \"({b}) & ~({a})\",\n",
    "    \"B\": \"{b}\",\n",
    "    \"A⊕B\": \"({a}) ^ ({b})\",\n",
    "    \"A∨B\": \"({a}) | ({b})\",\n",
    "    \"¬(A∨B)\": \"~(({a}) | ({b}))\",\n",
    "    \"¬(A⊕B)\": \"~(({a}) ^ ({b}))\",\n",
    "    \"¬B\": \"~({b})\",\n",
    "    \"B⇒A\": \"~({b}) | ({a})\",\n",
    "    \"¬A\": \"~({a})\",\n",
    "    \"A⇒B\": \"~({a}) | ({b})\",\n",
    "    \"¬(A∧B)\": \"~(({a}) & ({b}))\",\n",
    "    \"1\": \"1'b1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0a28141-16b1-4e15-8adc-a4aa7cb405cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "side = 16 # pixels in one side of the image\n",
    "N_input = 16 * 16  # number of input dimensions\n",
    "N_output = 10  # number of output dimensions (classes)\n",
    "\n",
    "# converts the learned logic gates to verilog or vhdl \n",
    "def generate_verilog(model, filename=\"logic_network.v\"):\n",
    "    \n",
    "    N_layers = len(model.logic_layers)\n",
    "\n",
    "    # gets number of neurons per layer\n",
    "    neurons_per_layer = [layer.weights.size()[0] for layer in model.logic_layers]\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        # module declaration\n",
    "        file.write(\"module logic_network(\\n\")\n",
    "        file.write(f\"    input wire [{N_input-1}:0] inputs,\\n\")\n",
    "        file.write(f\"    output wire [{N_output-1}:0] outputs\\n\")\n",
    "        file.write(\");\\n\\n\")\n",
    "\n",
    "        # declares wires for internal layers\n",
    "        for layer_index in range(N_layers - 1):\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "            file.write(f\"    wire [{N_neurons -1}:0] layer{layer_index}_outputs;\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "        logic_operations = list(logic_gate_verilog.keys())\n",
    "\n",
    "        for layer_index in range(N_layers):\n",
    "            logic_layer = model.logic_layers[layer_index]\n",
    "\n",
    "            # gets input and output indices\n",
    "            input_indices = logic_layer.indices[0].cpu().numpy()  # first input indices\n",
    "            output_indices = logic_layer.indices[1].cpu().numpy()  # second input indices\n",
    "\n",
    "            neuron_gates = [torch.argmax(logic_layer.weights[neuron]).item()\n",
    "                            for neuron in range(logic_layer.weights.size()[0])]\n",
    "            connections = {i: (input_indices[i], output_indices[i]) for i in range(len(neuron_gates))}\n",
    "\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "\n",
    "            # determines input wires\n",
    "            if layer_index == 0:\n",
    "                input_wire_base = \"inputs\"\n",
    "            else:\n",
    "                input_wire_base = f\"layer{layer_index -1}_outputs\"\n",
    "\n",
    "            # determines output wires\n",
    "            if layer_index == N_layers - 1:\n",
    "                output_wire_base = \"outputs\"\n",
    "            else:\n",
    "                output_wire_base = f\"layer{layer_index}_outputs\"\n",
    "\n",
    "            # assign statements for this layer\n",
    "            for neuron_id in range(N_neurons):\n",
    "                a_idx, b_idx = connections[neuron_id]\n",
    "\n",
    "                # maps indices to input wires\n",
    "                a_wire = f\"{input_wire_base}[{a_idx}]\"\n",
    "                b_wire = f\"{input_wire_base}[{b_idx}]\"\n",
    "\n",
    "                # gets gate\n",
    "                gate_op = logic_operations[neuron_gates[neuron_id]]\n",
    "                gate = logic_gate_verilog[gate_op].format(a=a_wire, b=b_wire)\n",
    "\n",
    "                # assigns to output wire\n",
    "                output_wire = f\"{output_wire_base}[{neuron_id}]\"\n",
    "\n",
    "                file.write(f\"    assign {output_wire} = {gate};\\n\")\n",
    "\n",
    "        file.write(\"endmodule\\n\")\n",
    "        print('success')\n",
    "\n",
    "# generates Verilog file for all trained models\n",
    "for model_idx in range(len(trained_models)):\n",
    "    i = model_idx + 1\n",
    "    generate_verilog(trained_models[model_idx], filename=f\"verilog/{side}x{side}/model_{i:03d}_logic_network.v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7325be-1e5f-4c16-8a49-a66327d183bd",
   "metadata": {},
   "source": [
    "#### VHDL Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22f5f2-059b-453e-b32e-9634cf8a9e3c",
   "metadata": {},
   "source": [
    "Logic gate to Verilog expression mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78b2f39a-e420-4b4d-93a6-1c0182bd15b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logic_gate_vhdl = {\n",
    "    \"0\": \"'0'\",\n",
    "    \"A∧B\": \"({a}) and ({b})\",\n",
    "    \"¬(A⇒B)\": \"({a}) and not ({b})\",\n",
    "    \"A\": \"{a}\",\n",
    "    \"¬(B⇒A)\": \"({b}) and not ({a})\",\n",
    "    \"B\": \"{b}\",\n",
    "    \"A⊕B\": \"({a}) xor ({b})\",\n",
    "    \"A∨B\": \"({a}) or ({b})\",\n",
    "    \"¬(A∨B)\": \"not(({a}) or ({b}))\",\n",
    "    \"¬(A⊕B)\": \"not(({a}) xor ({b}))\",\n",
    "    \"¬B\": \"not({b})\",\n",
    "    \"B⇒A\": \"not({b}) or ({a})\",\n",
    "    \"¬A\": \"not({a})\",\n",
    "    \"A⇒B\": \"not({a}) or ({b})\",\n",
    "    \"¬(A∧B)\": \"not(({a}) and ({b}))\",\n",
    "    \"1\": \"'1'\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20389672-5297-4dd4-a368-1b96df1445ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "side = 16 # pixels in one side of the image\n",
    "N_input = 16 * 16  # number of input dimensions\n",
    "N_output = 10  # number of output dimensions (classes)\n",
    "\n",
    "# Converts the learned logic gates to VHDL\n",
    "def generate_vhdl(model, filename=\"logic_network.vhdl\"):\n",
    "    N_layers = len(model.logic_layers)\n",
    "    neurons_per_layer = [layer.weights.size()[0] for layer in model.logic_layers]\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        # Library and entity declaration\n",
    "        file.write(\"library IEEE;\\n\")\n",
    "        file.write(\"use IEEE.STD_LOGIC_1164.ALL;\\n\\n\")\n",
    "        file.write(\"entity logic_network is\\n\")\n",
    "        file.write(f\"    port (\\n\")\n",
    "        file.write(f\"        inputs : in std_logic_vector({N_input - 1} downto 0);\\n\")\n",
    "        file.write(f\"        outputs : out std_logic_vector({N_output - 1} downto 0)\\n\")\n",
    "        file.write(\"    );\\n\")\n",
    "        file.write(\"end logic_network;\\n\\n\")\n",
    "        file.write(\"architecture Behavioral of logic_network is\\n\")\n",
    "\n",
    "        # Declare signals for internal layers\n",
    "        for layer_index in range(N_layers - 1):\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "            file.write(f\"    signal layer{layer_index}_outputs : std_logic_vector({N_neurons - 1} downto 0);\\n\")\n",
    "        file.write(\"\\nbegin\\n\\n\")\n",
    "\n",
    "        logic_operations = list(logic_gate_vhdl.keys())\n",
    "\n",
    "        # Generate VHDL code for each layer\n",
    "        for layer_index in range(N_layers):\n",
    "            logic_layer = model.logic_layers[layer_index]\n",
    "\n",
    "            # Get input and output indices\n",
    "            input_indices = logic_layer.indices[0].cpu().numpy()  # first input indices\n",
    "            output_indices = logic_layer.indices[1].cpu().numpy()  # second input indices\n",
    "\n",
    "            neuron_gates = [torch.argmax(logic_layer.weights[neuron]).item()\n",
    "                            for neuron in range(logic_layer.weights.size()[0])]\n",
    "            connections = {i: (input_indices[i], output_indices[i]) for i in range(len(neuron_gates))}\n",
    "\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "\n",
    "            # Determine input signals\n",
    "            if layer_index == 0:\n",
    "                input_wire_base = \"inputs\"\n",
    "            else:\n",
    "                input_wire_base = f\"layer{layer_index -1}_outputs\"\n",
    "\n",
    "            # Determine output signals\n",
    "            if layer_index == N_layers - 1:\n",
    "                output_wire_base = \"outputs\"\n",
    "            else:\n",
    "                output_wire_base = f\"layer{layer_index}_outputs\"\n",
    "\n",
    "            # Assign statements for each neuron in this layer\n",
    "            for neuron_id in range(N_neurons):\n",
    "                a_idx, b_idx = connections[neuron_id]\n",
    "\n",
    "                # Map indices to input signals\n",
    "                a_wire = f\"{input_wire_base}({a_idx})\"\n",
    "                b_wire = f\"{input_wire_base}({b_idx})\"\n",
    "\n",
    "                # Get the gate operation\n",
    "                gate_op = logic_operations[neuron_gates[neuron_id]]\n",
    "                gate = logic_gate_vhdl[gate_op].format(a=a_wire, b=b_wire)\n",
    "\n",
    "                # Assign to output signal\n",
    "                output_wire = f\"{output_wire_base}({neuron_id})\"\n",
    "                file.write(f\"    {output_wire} <= {gate};\\n\")\n",
    "\n",
    "        file.write(\"\\nend Behavioral;\\n\")\n",
    "        print('success')\n",
    "\n",
    "# Generates VHDL files for all trained models\n",
    "for model_idx in range(len(trained_models)):\n",
    "    i = model_idx + 1\n",
    "    generate_vhdl(trained_models[model_idx], filename=f\"vhdl/{side}x{side}/model_{i:03d}_logic_network.vhdl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d3597-9cf6-4688-a0ab-a676848f4439",
   "metadata": {},
   "source": [
    "#### Predicting from Hex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59df202-6740-4136-82ad-2ad266c20eb6",
   "metadata": {},
   "source": [
    "Sanity check to see that the FPGA output matches the model predictions on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec3f7637-d060-4831-a894-7ceed63368d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Binary string\n",
    "img = \"0000011111111111001111111111000000111111111100000001011100010000000001100000000000000111000000000000001100000000000000011100000000000000111000000000000001111000000000000001110000000000000111000000000001111100000000011111110000000011111100000000111111000000\"\n",
    "\n",
    "# Convert the binary string to a list of integers (0 and 1)\n",
    "img = [int(bit) for bit in img]\n",
    "\n",
    "# Convert list to a NumPy array\n",
    "img_array = np.array(img)\n",
    "\n",
    "# Reshape the array to 16x16\n",
    "reshaped = img_array.reshape(16, 16)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(reshaped, cmap='gray')\n",
    "plt.axis('off')  # Optional: Hide axis\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIFFLOGIC",
   "language": "python",
   "name": "difflogic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
