{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df65bb0b-594d-4717-bd3c-63b04b9d864b",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "fb5dd1c0-73cf-4679-ae21-6fa00a85a395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import yaml\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Compose, Lambda\n",
    "import mnist_dataset\n",
    "from hydra import initialize, compose\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, Subset\n",
    "from difflogic import LogicLayer, GroupSum\n",
    "from mnist_dataset import MNISTRemoveBorderTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99c41f-b5c7-4d64-a704-5a003a5b56c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8096d-fc17-46d7-861e-685d2fe12dbb",
   "metadata": {},
   "source": [
    "Tunable Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "47ea8409-a084-4bee-8121-96acd99bb28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configurable options\n",
    "remove_border = True       # True: Removes border of Mnist, False: Keeps black border around digits\n",
    "binarize_images = True     # True: Binarized Images, False: Grayscale Images \n",
    "evenly_partitioned = True  # True: Even distribution of samples, False: Original Mnist distribution\n",
    "upscaled_images = False    # True: Upscales the samples to 32x32, False: Keeps size unchanged\n",
    "downscaled_images = True   # True: Downscales the samples to 16x16, False: Keeps size unchanged\n",
    "batch_size = 256           # Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4450630-e7a8-4564-b106-6e2dc3a12e61",
   "metadata": {},
   "source": [
    "Dataset Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1779afcd-2def-41d1-abdc-c1bc8dbb1719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to binarize an image, threshold is tunable \n",
    "def binarize(image, threshold=0.5):\n",
    "    return (image > threshold).float()  \n",
    "\n",
    "# define the transformation logic based on the toggle\n",
    "transform_list = [ToTensor()]\n",
    "\n",
    "if remove_border:\n",
    "    transform_list.append(MNISTRemoveBorderTransform())\n",
    "    \n",
    "if upscaled_images:\n",
    "    transform_list.append(Resize((32, 32)))\n",
    "elif downscaled_images:\n",
    "    transform_list.append(Resize((16, 16)))\n",
    "if binarize_images:\n",
    "    transform_list.append(Lambda(lambda x: binarize(x)))\n",
    "    \n",
    "# adds binarization if enabled\n",
    "if binarize_images:\n",
    "    transform_list.append(Lambda(lambda x: binarize(x)))\n",
    "    \n",
    "transform = Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "9d9255d2-bf0c-4bab-b66a-b65360bcb86f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = mnist_dataset.MNIST('./data-mnist', train=True, download=True, transform=transform)\n",
    "test_dataset = mnist_dataset.MNIST('./data-mnist', train=False, transform=transform)\n",
    "\n",
    "# drop_last = True means it will drop the last incomplete Batch\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "38a5db70-d2ba-44a7-803c-829177a182c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes the Dataset evenly partitioned\n",
    "if evenly_partitioned:\n",
    "    # code below is used so that all classes have the same number of samples\n",
    "    train_targets = train_loader.dataset.targets\n",
    "    test_targets = test_loader.dataset.targets\n",
    "\n",
    "    train_digits_total = []\n",
    "    test_digits_total = []\n",
    "\n",
    "    for i in range(10):\n",
    "        curr_tot_train = torch.sum(train_targets == i).item()\n",
    "        curr_tot_test = torch.sum(test_targets == i).item()    \n",
    "        train_digits_total.append(curr_tot_train)\n",
    "        test_digits_total.append(curr_tot_test)\n",
    "\n",
    "    train_digits_total, test_digits_total\n",
    "\n",
    "    # find the minimum number of samples across all classes\n",
    "    min_samples_train = min(train_digits_total)\n",
    "    min_samples_test = min(test_digits_total)\n",
    "\n",
    "    # function to trim dataset to match the minimum samples for each class and shuffle indices\n",
    "    def trim_dataset(dataset, targets, min_samples):\n",
    "        indices = []\n",
    "        for i in range(10):\n",
    "            class_indices = (targets == i).nonzero(as_tuple=True)[0]  # get indices of class i\n",
    "            class_indices = class_indices[:min_samples]  # trim to min_samples\n",
    "            indices.extend(class_indices)\n",
    "\n",
    "        # shuffle indices after collecting them\n",
    "        indices = torch.tensor(indices)\n",
    "        indices = indices[torch.randperm(indices.size(0))]  \n",
    "\n",
    "        return Subset(dataset, indices)\n",
    "\n",
    "    # trim both train and test datasets to ensure all classes have the same number of samples\n",
    "    trimmed_train_dataset = trim_dataset(train_loader.dataset, train_targets, min_samples_train)\n",
    "    trimmed_test_dataset = trim_dataset(test_loader.dataset, test_targets, min_samples_test)\n",
    "\n",
    "    # create DataLoaders for the trimmed datasets\n",
    "    trimmed_train_loader = DataLoader(trimmed_train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    trimmed_test_loader = DataLoader(trimmed_test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "\n",
    "    # verify the lengths of the trimmed datasets\n",
    "    len(trimmed_train_loader.dataset), len(trimmed_test_loader.dataset)\n",
    "\n",
    "    train_dataset = trimmed_train_dataset\n",
    "    test_dataset = trimmed_test_dataset\n",
    "    train_loader = trimmed_train_loader\n",
    "    test_loader = trimmed_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "aebb9b7f-2399-4b9f-8a8a-570c922f92ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54210, 8920)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = len(train_loader.dataset) + len(test_loader.dataset)\n",
    "len(train_loader.dataset), len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "66230413-408e-4bc1-bf5f-2313690e0964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14c4138af580>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_size = len(train_dataset)\n",
    "random_index = random.randint(0, dataset_size - 1)\n",
    "\n",
    "if remove_border and not downscaled_images:\n",
    "    image = train_loader.dataset[random_index][0].reshape(20, 20)\n",
    "elif not remove_border and not upscaled_images and not downscaled_images:\n",
    "    image = train_loader.dataset[random_index][0].reshape(28, 28)\n",
    "elif downscaled_images:\n",
    "    image = train_loader.dataset[random_index][0].reshape(16, 16)\n",
    "else:\n",
    "    image = train_loader.dataset[random_index][0].reshape(32, 32)\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973f0e3-5f32-4793-afc6-9394425419de",
   "metadata": {},
   "source": [
    "#### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e63be-c489-4aaf-a605-9e1d0fda631f",
   "metadata": {},
   "source": [
    "Converts csv into yaml config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a9b89fd2-f200-40c1-973a-489ef4190e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define first input and the name of the file to be saved\n",
    "first_in_dim = 256 # 16x16\n",
    "filename = \"config/mnist_config_16x16.yaml\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "4dc64546-f1d7-4e18-b88c-7140d4d42692",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML file 'config/mnist_config_16x16.yaml' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# reads the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"config/hyperparameters.csv\")\n",
    "\n",
    "# convert the DataFrame to a list of dictionaries\n",
    "models = df.to_dict(orient=\"records\")\n",
    "\n",
    "# create the YAML structure\n",
    "yaml_structure = {\"models\": {}}\n",
    "\n",
    "# rounds the number to the nearest multiple of the output size\n",
    "def round_to_nearest_multiple(value, multiple):\n",
    "    return multiple * round(value / multiple)\n",
    "\n",
    "# populate the YAML structure with models\n",
    "for i, model in enumerate(models, start=1):\n",
    "    # zero-padding model names to 3 digits \n",
    "    model_name = f\"model_{str(i).zfill(3)}\"\n",
    "    layers_config = {}\n",
    "    \n",
    "    for layer in range(1, model[\"H\"] + 1):\n",
    "        # zero-padding the layer names to 3 digits\n",
    "        layer_name = f\"LogicLayer{str(layer).zfill(3)}\"\n",
    "        \n",
    "        # adjusts in_dim to the nearest multiple of 10\n",
    "        in_dim = first_in_dim if layer == 1 else round_to_nearest_multiple(model[\"W\"], 10)\n",
    "        \n",
    "        # adjusts out_dim to the nearest multiple of 10\n",
    "        out_dim = round_to_nearest_multiple(model[\"W\"], 10)\n",
    "        \n",
    "        layers_config[layer_name] = {\n",
    "            \"in_dim\": in_dim,\n",
    "            \"out_dim\": out_dim,\n",
    "            \"device\": \"cuda\",\n",
    "            \"implementation\": \"cuda\",\n",
    "            \"connections\": \"random\",\n",
    "            \"grad_factor\": 2, # we can try different grad_factor values as well\n",
    "        }\n",
    "    \n",
    "    yaml_structure[\"models\"][model_name] = {\n",
    "        \"input_dim\": first_in_dim, \n",
    "        \"output_size\": 10, # for MNIST classification\n",
    "        \"tau\": model[\"tau\"],\n",
    "        \"learning_rate\": model[\"lr\"],\n",
    "        \"layers_config\": layers_config,\n",
    "    }\n",
    "\n",
    "# saves to a YAML file\n",
    "with open(f'{filename}', \"w\") as file:\n",
    "    yaml.dump(yaml_structure, file, default_flow_style=False)\n",
    "\n",
    "print(f\"YAML file '{filename}' generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e939ace-e4c3-4781-bdea-f9722ddb925e",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473370c-ec74-455b-854c-f597d04144f3",
   "metadata": {},
   "source": [
    "DiffLogic Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b244edec-9bd5-474d-9aaa-4f9aafe31b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffLogic(nn.Module):\n",
    "    def __init__(self, layers_config, output_size, tau=30):\n",
    "        \"\"\"\n",
    "        Initializes the DiffLogic model with the specified layer configurations, output size, and temperature parameter.\n",
    "\n",
    "        Args:\n",
    "            layers_config (dict): Configuration for each logic layer, including dimensions, device, implementation, connections, and grad factor.\n",
    "            output_size (int): The number of output groups (classes in a classification problem).\n",
    "            tau (int): Temperature parameter for the GroupSum operation.\n",
    "        \"\"\"\n",
    "        super(DiffLogic, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # stores the logic layers\n",
    "        layers = []\n",
    "        for layer_name, config in layers_config.items():\n",
    "            layer = LogicLayer(\n",
    "                in_dim=config['in_dim'],\n",
    "                out_dim=config['out_dim'],\n",
    "                device=config['device'],\n",
    "                implementation=config['implementation'],\n",
    "                connections=config['connections'],\n",
    "                grad_factor=config['grad_factor']       \n",
    "            )\n",
    "            layers.append(layer)\n",
    "            print(layer)\n",
    "        \n",
    "        self.logic_layers = nn.Sequential(*layers)\n",
    "        self.group = GroupSum(k=output_size, tau=tau)\n",
    "        self.log_text = \"\"  # initializes logging string\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the DiffLogic model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after processing through the logic layers and grouping operation.\n",
    "        \"\"\"\n",
    "        # moves tensor to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to('cuda')          \n",
    "        x = self.flatten(x)\n",
    "        logits = self.logic_layers(x)\n",
    "        group = self.group(logits)\n",
    "        return group\n",
    "    \n",
    "    def save(self, file_path, model_name='model'):\n",
    "        \"\"\"\n",
    "        Saves the model's state dictionary to the specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path where the model will be saved.\n",
    "            model_name (str): Name of the saved model\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'connections': [layer.indices for layer in self.logic_layers if isinstance(layer, LogicLayer)]\n",
    "        }, os.path.join(file_path, f\"{model_name}.pth\"))\n",
    "        self.log_text += f\"Model saved to: {file_path}\\n\"\n",
    "\n",
    "    def load(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads the model's state dictionary from the specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path from which the model will be loaded.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(file_path)\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # assigns connections to each LogicLayer\n",
    "        for idx, layer in enumerate(self.logic_layers):\n",
    "            if isinstance(layer, LogicLayer):\n",
    "                layer.indices = checkpoint['connections'][idx]\n",
    "\n",
    "        self.eval()\n",
    "        self.log_text += f\"Model loaded from: {file_path}\\n\"\n",
    "        \n",
    "    def get_accuracy(self, data_loader):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy of the model against a data loader\n",
    "\n",
    "        Args:\n",
    "            data_loader: a DataLoader object, e.g. train_loader or test_loader\n",
    "\n",
    "        Returns:\n",
    "            float: The accuracy\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # ensures that model is in evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            for batch_inputs, batch_outputs in tqdm(data_loader, desc=\"Running Inference\"):\n",
    "                batch_inputs, batch_outputs = batch_inputs.to('cuda'), batch_outputs.to('cuda')\n",
    "\n",
    "                # forward pass to get predictions\n",
    "                outputs = self(batch_inputs)\n",
    "\n",
    "                # gets the predicted class (index of the maximum logit)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # counting correct predictions\n",
    "                total += batch_outputs.size(0)  # total number of samples in the batch\n",
    "                correct += (predicted == batch_outputs).sum().item()  # counting correct predictions\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return accuracy\n",
    "\n",
    "    def get_log(self):\n",
    "        \"\"\"\n",
    "        Retrieves the log text and clears the log after retrieval.\n",
    "\n",
    "        Returns:\n",
    "            str: The log text.\n",
    "        \"\"\"\n",
    "        log_copy = self.log_text\n",
    "        self.log_text = \"\"  # Clear the log after returning\n",
    "        return log_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "5632fa27-5e05-41b0-b3a3-ac941714657b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        \"\"\"\n",
    "        Initializes the EarlyStopper to stop training if the performance doesn't improve after a certain number of epochs.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait for an improvement.\n",
    "            min_delta (float): Minimum change to consider an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def should_stop(self, current_loss):\n",
    "        \"\"\"\n",
    "        Check if training should stop based on the current loss.\n",
    "\n",
    "        Args:\n",
    "            current_loss (float): The current loss.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if training should stop, False otherwise.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = current_loss\n",
    "            return False\n",
    "        elif current_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = current_loss\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(\"EarlyStopper Triggered: \", self.counter)\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd97e0b-c6c4-4262-97ab-32136126ba39",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea90015-6e45-49af-bb11-539c8811adcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize Hydra with the config path and job name\n",
    "with initialize(version_base=None, config_path=\"config\", job_name=\"aidays2024\"):\n",
    "    cfg = compose(config_name=\"mnist_config_16x16\")\n",
    "\n",
    "# training loop for all models\n",
    "all_models_dict = {}\n",
    "num_epochs = 50\n",
    "file_path = 'trained_models/mnist_trained_16x16' # where to save your trained models\n",
    "\n",
    "# loops through all model configs and trains each of them\n",
    "for model_name, model_cfg in cfg.models.items():\n",
    "    print(f'training model {model_name}')\n",
    "\n",
    "    # tracking dictionary\n",
    "    all_models_dict[model_name] = {\n",
    "        'losses': [],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # initializes DiffLogic model and moves to CUDA if available\n",
    "        model = DiffLogic(layers_config=model_cfg['layers_config'], \n",
    "                          output_size=model_cfg['output_size'], \n",
    "                          tau=model_cfg['tau']).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # optimizer and loss criterion\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=model_cfg['learning_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # early stopping\n",
    "        early_stopper = EarlyStopper(patience=5)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            loop = tqdm(train_loader, leave=True, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "            epoch_loss = 0  # to track loss for an epoch\n",
    "            \n",
    "            for batch_inputs, batch_outputs in loop:\n",
    "                # move data to the appropriate device\n",
    "                device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "                batch_inputs, batch_outputs = batch_inputs.to(device).double(), batch_outputs.to(device).long()\n",
    "\n",
    "                # forward pass through the model\n",
    "                predictions = model(batch_inputs)\n",
    "                loss = criterion(predictions, batch_outputs)\n",
    "\n",
    "                # zero gradients, backpropagates, and updates model parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # accumulating the loss for the epoch\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # caclulating the average loss for the epoch\n",
    "            epoch_loss /= len(train_loader)\n",
    "            all_models_dict[model_name]['losses'].append(epoch_loss)\n",
    "            print(f'Epoch {epoch+1} Loss: {epoch_loss}')\n",
    "\n",
    "            # checks for early stopping\n",
    "            if early_stopper.should_stop(epoch_loss):\n",
    "                print(f\"Early stopping triggered for {model_name} at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "        # saving trained model's state\n",
    "        model.save(file_path, model_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR TRAINING {model_name.upper()}: {str(e)}\")\n",
    "\n",
    "print(\"All models processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28a834-adac-4241-83a4-d9a23ac17db5",
   "metadata": {},
   "source": [
    "#### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "33048f62-fc21-4f87-b725-6eb470a66700",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "Evaluating model_001.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:08<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_001.pth: 83.74%\n",
      "\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "Evaluating model_002.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 14.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_002.pth: 89.05%\n",
      "\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "Evaluating model_003.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_003.pth: 90.97%\n",
      "\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "Evaluating model_004.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_004.pth: 92.60%\n",
      "\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "Evaluating model_005.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_005.pth: 92.88%\n",
      "\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "Evaluating model_006.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_006.pth: 86.66%\n",
      "\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "Evaluating model_007.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_007.pth: 90.54%\n",
      "\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "Evaluating model_008.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_008.pth: 92.90%\n",
      "\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "Evaluating model_009.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_009.pth: 93.41%\n",
      "\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "Evaluating model_010.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_010.pth: 94.30%\n",
      "\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "Evaluating model_011.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_011.pth: 86.05%\n",
      "\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "Evaluating model_012.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_012.pth: 90.97%\n",
      "\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "Evaluating model_013.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_013.pth: 92.56%\n",
      "\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "Evaluating model_014.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_014.pth: 93.90%\n",
      "\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "Evaluating model_015.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_015.pth: 94.45%\n",
      "\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "Evaluating model_016.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_016.pth: 85.89%\n",
      "\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "Evaluating model_017.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_017.pth: 89.83%\n",
      "\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "Evaluating model_018.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_018.pth: 91.77%\n",
      "\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "Evaluating model_019.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_019.pth: 93.26%\n",
      "\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "Evaluating model_020.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_020.pth: 93.69%\n",
      "\n",
      "LogicLayer(256, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "LogicLayer(2560, 2560, train)\n",
      "Evaluating model_021.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_021.pth: 82.10%\n",
      "\n",
      "LogicLayer(256, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "LogicLayer(5120, 5120, train)\n",
      "Evaluating model_022.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_022.pth: 87.76%\n",
      "\n",
      "LogicLayer(256, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "LogicLayer(7680, 7680, train)\n",
      "Evaluating model_023.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_023.pth: 89.84%\n",
      "\n",
      "LogicLayer(256, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "LogicLayer(10240, 10240, train)\n",
      "Evaluating model_024.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 14.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_024.pth: 91.37%\n",
      "\n",
      "LogicLayer(256, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "LogicLayer(12800, 12800, train)\n",
      "Evaluating model_025.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:02<00:00, 15.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_025.pth: 90.98%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# testing loop to test inferences\n",
    "trained_models_dir = 'trained_models/mnist_trained_16x16'\n",
    "\n",
    "# retrieves a list of all model files in the directory\n",
    "model_files = sorted([f for f in os.listdir(trained_models_dir) if f.endswith('.pth')])\n",
    "\n",
    "with initialize(version_base=None, config_path=\"config\", job_name=\"test_app\"):\n",
    "    cfg = compose(config_name=\"mnist_config_16x16\")\n",
    "\n",
    "# dictionary to store the trained models\n",
    "trained_models = {}\n",
    "\n",
    "# loops through all model files and calculates their accuracies\n",
    "for i, model_file in enumerate(model_files):\n",
    "    if model_file.endswith('_weights.pth'):\n",
    "        model_name = model_file.removesuffix('_weights.pth')\n",
    "    else:\n",
    "        model_name = model_file.removesuffix('.pth')\n",
    "    \n",
    "    model_cfg = cfg['models'][model_name]\n",
    "    \n",
    "    # instantiates the model and load its weights\n",
    "    model = DiffLogic(layers_config=model_cfg['layers_config'], \n",
    "                          output_size=model_cfg['output_size'], \n",
    "                          tau=model_cfg['tau']).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_path = os.path.join(trained_models_dir, model_file)\n",
    "    print(f\"Evaluating {model_file}...\")\n",
    "\n",
    "    # loads the respective model\n",
    "    model.load(model_path)\n",
    "\n",
    "    # calculates accuracy\n",
    "    accuracy = model.get_accuracy(test_loader)\n",
    "    \n",
    "    print(f\"Accuracy of {model_file}: {accuracy * 100:.2f}%\\n\")\n",
    "    \n",
    "    trained_models[i] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd2b5d-ed32-4ff0-9c40-caa7169ae1ed",
   "metadata": {},
   "source": [
    "#### Model Optimization (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8665f3c-8c20-400e-a147-d92abac655eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# maybe remove the unused nodes to increase inference speed / decrease energy consumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c06262-afe8-4b02-883c-c044a060fc39",
   "metadata": {},
   "source": [
    "#### Verilog Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e20496-c3ac-4d87-8e55-359ac0069b03",
   "metadata": {},
   "source": [
    "Logic gate to Verilog expression mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2846e8b8-9b6f-49dc-adbc-7bfe8fe0a27d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logic_gate_verilog = {\n",
    "    \"0\": \"1'b0\",\n",
    "    \"A∧B\": \"({a}) & ({b})\",\n",
    "    \"¬(A⇒B)\": \"({a}) & ~({b})\",\n",
    "    \"A\": \"{a}\",\n",
    "    \"¬(B⇒A)\": \"({b}) & ~({a})\",\n",
    "    \"B\": \"{b}\",\n",
    "    \"A⊕B\": \"({a}) ^ ({b})\",\n",
    "    \"A∨B\": \"({a}) | ({b})\",\n",
    "    \"¬(A∨B)\": \"~(({a}) | ({b}))\",\n",
    "    \"¬(A⊕B)\": \"~(({a}) ^ ({b}))\",\n",
    "    \"¬B\": \"~({b})\",\n",
    "    \"B⇒A\": \"~({b}) | ({a})\",\n",
    "    \"¬A\": \"~({a})\",\n",
    "    \"A⇒B\": \"~({a}) | ({b})\",\n",
    "    \"¬(A∧B)\": \"~(({a}) & ({b}))\",\n",
    "    \"1\": \"1'b1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "f0a28141-16b1-4e15-8adc-a4aa7cb405cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "side = 16 # pixels in one side of the image\n",
    "N_input = 16 * 16  # number of input dimensions\n",
    "\n",
    "# converts the learned logic gates to verilog or vhdl \n",
    "def generate_verilog(model, filename=\"logic_network.v\"):\n",
    "    \n",
    "    N_layers = len(model.logic_layers)\n",
    "\n",
    "    # gets number of neurons per layer\n",
    "    neurons_per_layer = [layer.weights.size()[0] for layer in model.logic_layers]\n",
    "    \n",
    "    # Set the output size to the number of neurons in the last layer\n",
    "    N_output = neurons_per_layer[-1]\n",
    "    \n",
    "    with open(filename, 'w') as file:\n",
    "        # module declaration\n",
    "        file.write(\"module logic_network(\\n\")\n",
    "        file.write(f\"    input wire [{N_input-1}:0] inputs,\\n\")\n",
    "        file.write(f\"    output wire [{N_output-1}:0] outputs\\n\")\n",
    "        file.write(\");\\n\\n\")\n",
    "\n",
    "        # declares wires for internal layers\n",
    "        for layer_index in range(N_layers - 1):\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "            file.write(f\"    wire [{N_neurons -1}:0] layer{layer_index}_outputs;\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "        logic_operations = list(logic_gate_verilog.keys())\n",
    "\n",
    "        for layer_index in range(N_layers):\n",
    "            logic_layer = model.logic_layers[layer_index]\n",
    "\n",
    "            # gets input and output indices\n",
    "            input_indices = logic_layer.indices[0].cpu().numpy()  # first input indices\n",
    "            output_indices = logic_layer.indices[1].cpu().numpy()  # second input indices\n",
    "\n",
    "            neuron_gates = [torch.argmax(logic_layer.weights[neuron]).item()\n",
    "                            for neuron in range(logic_layer.weights.size()[0])]\n",
    "            connections = {i: (input_indices[i], output_indices[i]) for i in range(len(neuron_gates))}\n",
    "\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "\n",
    "            # determines input wires\n",
    "            if layer_index == 0:\n",
    "                input_wire_base = \"inputs\"\n",
    "            else:\n",
    "                input_wire_base = f\"layer{layer_index -1}_outputs\"\n",
    "\n",
    "            # determines output wires\n",
    "            if layer_index == N_layers - 1:\n",
    "                output_wire_base = \"outputs\"\n",
    "            else:\n",
    "                output_wire_base = f\"layer{layer_index}_outputs\"\n",
    "\n",
    "            # assign statements for this layer\n",
    "            for neuron_id in range(N_neurons):\n",
    "                a_idx, b_idx = connections[neuron_id]\n",
    "\n",
    "                # maps indices to input wires\n",
    "                a_wire = f\"{input_wire_base}[{a_idx}]\"\n",
    "                b_wire = f\"{input_wire_base}[{b_idx}]\"\n",
    "\n",
    "                # gets gate\n",
    "                gate_op = logic_operations[neuron_gates[neuron_id]]\n",
    "                gate = logic_gate_verilog[gate_op].format(a=a_wire, b=b_wire)\n",
    "\n",
    "                # assigns to output wire\n",
    "                output_wire = f\"{output_wire_base}[{neuron_id}]\"\n",
    "\n",
    "                file.write(f\"    assign {output_wire} = {gate};\\n\")\n",
    "\n",
    "        file.write(\"endmodule\\n\")\n",
    "        print('success')\n",
    "\n",
    "# generates Verilog file for all trained models\n",
    "for model_idx in range(len(trained_models)):\n",
    "    i = model_idx + 1\n",
    "    generate_verilog(trained_models[model_idx], filename=f\"verilog/{side}x{side}/model_{i:03d}_logic_network.v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7325be-1e5f-4c16-8a49-a66327d183bd",
   "metadata": {},
   "source": [
    "#### VHDL Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22f5f2-059b-453e-b32e-9634cf8a9e3c",
   "metadata": {},
   "source": [
    "Logic gate to Verilog expression mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "78b2f39a-e420-4b4d-93a6-1c0182bd15b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logic_gate_vhdl = {\n",
    "    \"0\": \"'0'\",\n",
    "    \"A∧B\": \"({a}) and ({b})\",\n",
    "    \"¬(A⇒B)\": \"({a}) and not ({b})\",\n",
    "    \"A\": \"{a}\",\n",
    "    \"¬(B⇒A)\": \"({b}) and not ({a})\",\n",
    "    \"B\": \"{b}\",\n",
    "    \"A⊕B\": \"({a}) xor ({b})\",\n",
    "    \"A∨B\": \"({a}) or ({b})\",\n",
    "    \"¬(A∨B)\": \"not(({a}) or ({b}))\",\n",
    "    \"¬(A⊕B)\": \"not(({a}) xor ({b}))\",\n",
    "    \"¬B\": \"not({b})\",\n",
    "    \"B⇒A\": \"not({b}) or ({a})\",\n",
    "    \"¬A\": \"not({a})\",\n",
    "    \"A⇒B\": \"not({a}) or ({b})\",\n",
    "    \"¬(A∧B)\": \"not(({a}) and ({b}))\",\n",
    "    \"1\": \"'1'\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "20389672-5297-4dd4-a368-1b96df1445ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "side = 16 # pixels in one side of the image\n",
    "N_input = 16 * 16  # number of input dimensions\n",
    "\n",
    "# Converts the learned logic gates to VHDL\n",
    "def generate_vhdl(model, filename=\"logic_network.vhdl\"):\n",
    "    N_layers = len(model.logic_layers)\n",
    "    neurons_per_layer = [layer.weights.size()[0] for layer in model.logic_layers]\n",
    "\n",
    "    # set output size to the number of neurons in the last layer\n",
    "    N_output = neurons_per_layer[-1]\n",
    "    \n",
    "    with open(filename, 'w') as file:\n",
    "        # Library and entity declaration\n",
    "        file.write(\"library IEEE;\\n\")\n",
    "        file.write(\"use IEEE.STD_LOGIC_1164.ALL;\\n\\n\")\n",
    "        file.write(\"entity logic_network is\\n\")\n",
    "        file.write(f\"    port (\\n\")\n",
    "        file.write(f\"        inputs : in std_logic_vector({N_input - 1} downto 0);\\n\")\n",
    "        file.write(f\"        outputs : out std_logic_vector({N_output - 1} downto 0)\\n\")\n",
    "        file.write(\"    );\\n\")\n",
    "        file.write(\"end logic_network;\\n\\n\")\n",
    "        file.write(\"architecture Behavioral of logic_network is\\n\")\n",
    "\n",
    "        # Declare signals for internal layers\n",
    "        for layer_index in range(N_layers - 1):\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "            file.write(f\"    signal layer{layer_index}_outputs : std_logic_vector({N_neurons - 1} downto 0);\\n\")\n",
    "        file.write(\"\\nbegin\\n\\n\")\n",
    "\n",
    "        logic_operations = list(logic_gate_vhdl.keys())\n",
    "\n",
    "        # Generate VHDL code for each layer\n",
    "        for layer_index in range(N_layers):\n",
    "            logic_layer = model.logic_layers[layer_index]\n",
    "\n",
    "            # Get input and output indices\n",
    "            input_indices = logic_layer.indices[0].cpu().numpy()  # first input indices\n",
    "            output_indices = logic_layer.indices[1].cpu().numpy()  # second input indices\n",
    "\n",
    "            neuron_gates = [torch.argmax(logic_layer.weights[neuron]).item()\n",
    "                            for neuron in range(logic_layer.weights.size()[0])]\n",
    "            connections = {i: (input_indices[i], output_indices[i]) for i in range(len(neuron_gates))}\n",
    "\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "\n",
    "            # Determine input signals\n",
    "            if layer_index == 0:\n",
    "                input_wire_base = \"inputs\"\n",
    "            else:\n",
    "                input_wire_base = f\"layer{layer_index -1}_outputs\"\n",
    "\n",
    "            # Determine output signals\n",
    "            if layer_index == N_layers - 1:\n",
    "                output_wire_base = \"outputs\"\n",
    "            else:\n",
    "                output_wire_base = f\"layer{layer_index}_outputs\"\n",
    "\n",
    "            # Assign statements for each neuron in this layer\n",
    "            for neuron_id in range(N_neurons):\n",
    "                a_idx, b_idx = connections[neuron_id]\n",
    "\n",
    "                # Map indices to input signals\n",
    "                a_wire = f\"{input_wire_base}({a_idx})\"\n",
    "                b_wire = f\"{input_wire_base}({b_idx})\"\n",
    "\n",
    "                # Get the gate operation\n",
    "                gate_op = logic_operations[neuron_gates[neuron_id]]\n",
    "                gate = logic_gate_vhdl[gate_op].format(a=a_wire, b=b_wire)\n",
    "\n",
    "                # Assign to output signal\n",
    "                output_wire = f\"{output_wire_base}({neuron_id})\"\n",
    "                file.write(f\"    {output_wire} <= {gate};\\n\")\n",
    "\n",
    "        file.write(\"\\nend Behavioral;\\n\")\n",
    "        print('success')\n",
    "\n",
    "# Generates VHDL files for all trained models\n",
    "for model_idx in range(len(trained_models)):\n",
    "    i = model_idx + 1\n",
    "    generate_vhdl(trained_models[model_idx], filename=f\"vhdl/{side}x{side}/model_{i:03d}_logic_network.vhdl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d3597-9cf6-4688-a0ab-a676848f4439",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Predicting from Hex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59df202-6740-4136-82ad-2ad266c20eb6",
   "metadata": {},
   "source": [
    "Sanity check to see that the FPGA output matches the model predictions on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "dac4a4f5-4b7f-47be-bf77-6864016839c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid line: 102 : 1.8001800180018E+62;\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def load_images_from_hex_file(filename):\n",
    "    images = []\n",
    "    hex_pattern = re.compile(r'^[0-9a-fA-F]+$')  # only allows valid hex characters\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if ':' in line:\n",
    "                hex_data = line.split(':')[1].strip().strip(';')\n",
    "                \n",
    "                # check if hex_data is a valid hexadecimal string\n",
    "                if hex_pattern.match(hex_data):\n",
    "                    # convert to binary and pad to 256 bits\n",
    "                    bin_data = bin(int(hex_data, 16))[2:].zfill(256)\n",
    "                    image = np.array([int(bit) for bit in bin_data], dtype=np.uint8).reshape(16, 16)\n",
    "                    images.append(image)\n",
    "                else:\n",
    "                    print(f\"Skipping invalid line: {line.strip()}\")\n",
    "\n",
    "    return np.array(images)\n",
    "\n",
    "# loading images\n",
    "filename = 'mnist_input.mif'  \n",
    "images = load_images_from_hex_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a62a3e-3426-4430-a11e-3f49cf0a582a",
   "metadata": {},
   "source": [
    "Displays a random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ec3f7637-d060-4831-a894-7ceed63368d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = random.randint(0, 1024)\n",
    "\n",
    "if idx == 102: # line that is broken\n",
    "    idx = 0\n",
    "\n",
    "# plot the image\n",
    "plt.imshow(images[idx], cmap='gray')\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "fe7da784-e8c6-47b5-af6f-4c8b09e1dcbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAPwUlEQVR4nO3be4iVdf7A8c+xUWfGcUtqbLVkmiy7WBEZ0V3dbjS11R8RLVtYEU1Xs1jswtJmDkTtbhpl1z8SRCiDtSAsSVJI/9jtYpGSJKJRCZlhml0cxvn+/hA/v8bxMl2co83rBQPNc77nOZ+n7Lx9nvOcSimlBABERL9qDwDAvkMUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkU6BVHHHFEXHfddfn7okWLolKpxKJFi6o20452nLE3jBs3Lk444YTfdJ/VOA5+P0ShD5g5c2ZUKpX8qa2tjVGjRsXtt98eX375ZbXH+1nmzZsXDz74YFVnqFQqcfvtt1d1ht4ye/bsqFQq0dDQUO1R6CU11R6A3vPQQw9Fc3Nz/Pjjj7F48eJ4+umnY968ebFs2bKor6/v1VnOPffc+OGHH2LAgAE/63nz5s2LGTNmVD0MfcHmzZtj8uTJMWjQoGqPQi9yptCHXHzxxXHNNdfEjTfeGDNnzoxJkybF6tWr49VXX93lc7777ru9Mku/fv2itrY2+vXzR3Bf1dbWFoMHD44rrrii2qPQi/wf2Yf96U9/ioiI1atXR0TEddddFw0NDbFq1apoaWmJwYMHx1//+teIiOjs7Izp06fH6NGjo7a2Ng499NBobW2NDRs2dNlnKSXa2tri8MMPj/r6+hg/fnwsX76822vv6jOF//73v9HS0hJDhgyJQYMGxUknnRSPP/54zjdjxoyIiC6Xw7b7rWf8NV599dW45JJLYvjw4TFw4MAYOXJkTJ06NbZu3brT9e+9916ceeaZUVdXF83NzfHMM890W7Nly5b4xz/+EUcddVQMHDgwRowYEZMnT44tW7bscZ5Vq1bFqlWrejz/ypUrY9q0afHYY49FTY0LCn2J/9p92PY3iYMPPji3dXR0xEUXXRRnn312/Otf/8rLSq2trTFz5sy4/vrrY+LEibF69ep48sknY+nSpbFkyZLo379/REQ88MAD0dbWFi0tLdHS0hLvv/9+XHjhhdHe3r7Hed5888249NJLY9iwYXHnnXfGH//4x/j444/jtddeizvvvDNaW1tj7dq18eabb8asWbO6Pb83ZuypmTNnRkNDQ9x9993R0NAQb731VjzwwAOxadOm+Oc//9ll7YYNG6KlpSWuuuqq+Mtf/hJz5syJW265JQYMGBA33HBDRGwL3mWXXRaLFy+Om266KY477rj46KOPYtq0afHJJ5/EK6+8stt5zjvvvIiIWLNmTY/mnzRpUowfPz5aWlpizpw5P/v42Y8VfvdeeOGFEhFlwYIF5auvviqfffZZefHFF8vBBx9c6urqyueff15KKWXChAklIsq9997b5flvv/12iYgye/bsLtvfeOONLtvXrVtXBgwYUC655JLS2dmZ6+6///4SEWXChAm5beHChSUiysKFC0sppXR0dJTm5ubS1NRUNmzY0OV1frqv2267rezsj+3emHFXIqLcdtttu13z/fffd9vW2tpa6uvry48//pjbxo4dWyKi/Pvf/85tW7ZsKSeffHIZOnRoaW9vL6WUMmvWrNKvX7/y9ttvd9nnM888UyKiLFmyJLc1NTV1O46mpqbS1NS0x2MrpZTXXnut1NTUlOXLl5dStv25GDRoUI+ey/7P5aM+5Pzzz4/GxsYYMWJEXH311dHQ0BBz586Nww47rMu6W265pcvvL7/8chx44IFxwQUXxPr16/NnzJgx0dDQEAsXLoyIiAULFkR7e3vccccdXS7rTJo0aY+zLV26NFavXh2TJk2Kgw46qMtjP93XrvTGjD9HXV1d/vO3334b69evj3POOSe+//77WLFiRZe1NTU10dramr8PGDAgWltbY926dfHee+/l8R133HFx7LHHdjm+7ZcAtx/frqxZs6ZHZwnt7e1x1113xc033xzHH398Tw+X3xGXj/qQGTNmxKhRo6KmpiYOPfTQOOaYY7p90FtTUxOHH354l20rV66MjRs3xtChQ3e633Xr1kVExKeffhoREUcffXSXxxsbG2PIkCG7nW37paxfes9+b8z4cyxfvjz+/ve/x1tvvRWbNm3q8tjGjRu7/D58+PBud/iMGjUqIra9mZ9++umxcuXK+Pjjj6OxsXGnr7f9+H6tadOmxfr162PKlCm/yf7Y/4hCH3LaaafFqaeeuts1AwcO7BaKzs7OGDp0aMyePXunz9nVG1Vv2pdm/Oabb2Ls2LHxhz/8IR566KEYOXJk1NbWxvvvvx/33HNPdHZ2/ux9dnZ2xoknnhiPPfbYTh8fMWLErx07Nm7cGG1tbXHrrbfGpk2bMmabN2+OUkqsWbMm6uvrdxlefh9EgT0aOXJkLFiwIM4666wul0V21NTUFBHb/tZ+5JFH5vavvvqq2x1AO3uNiIhly5bF+eefv8t1u7qU1Bsz9tSiRYvi66+/jv/85z9x7rnn5vbtd3ntaO3atfHdd991OVv45JNPImLbt5Mjth3fhx9+GOedd16PLqf9Ehs2bIjNmzfHo48+Go8++mi3x5ubm+Pyyy/f44fa7N98psAeXXXVVbF169aYOnVqt8c6Ojrim2++iYhtn1n0798/nnjiiSil5Jrp06fv8TVOOeWUaG5ujunTp+f+tvvpvra/ce64pjdm7KkDDjig29zt7e3x1FNP7XR9R0dHPPvss13WPvvss9HY2BhjxoyJiG3H98UXX8Tzzz/f7fk//PDDHr9P0pNbUocOHRpz587t9jN+/Piora2NuXPnxn333bfbfbD/c6bAHo0dOzZaW1vj4Ycfjg8++CAuvPDC6N+/f6xcuTJefvnlePzxx+PKK6+MxsbG+Nvf/hYPP/xwXHrppdHS0hJLly6N119/PQ455JDdvka/fv3i6aefjj//+c9x8sknx/XXXx/Dhg2LFStWxPLly2P+/PkREfkmOXHixLjooovigAMOiKuvvrpXZvypd999N9ra2rptHzduXJx55pkxZMiQmDBhQkycODEqlUrMmjWrSyR+avjw4fHII4/EmjVrYtSoUfHSSy/FBx98EM8991zeRnvttdfGnDlz4uabb46FCxfGWWedFVu3bo0VK1bEnDlzYv78+bu9NNiTW1Lr6+t3+kW1V155Jf73v//5EltfUc1bn+gd229Jfeedd3a7bk+3Hj733HNlzJgxpa6urgwePLiceOKJZfLkyWXt2rW5ZuvWrWXKlCll2LBhpa6urowbN64sW7as222SO96Sut3ixYvLBRdcUAYPHlwGDRpUTjrppPLEE0/k4x0dHeWOO+4ojY2NpVKpdLs99beccVciYpc/U6dOLaWUsmTJknL66aeXurq6Mnz48DJ58uQyf/78bsc8duzYMnr06PLuu++WM844o9TW1pampqby5JNPdnvd9vb28sgjj5TRo0eXgQMHliFDhpQxY8aUKVOmlI0bN+a6X3tL6o7cktq3VErZxV9fAOhzfKYAQBIFAJIoAJBEAYAkCgAkUQAg9fjLa3vrq/UA/L9qf0vAmQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYBUU+0BAPY3pZRqj7DXOFMAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQBSTbUHANgbSinVHuEXqVQqe23fPfl34kwBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABINdUeAOibSinVHoGdcKYAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAg1VR7AGDfVUqp9gj7pEqlUu0R9hpnCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAFJNtQfg96eUUu0RgF/ImQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYBU09OFpZS9OQf8rlUqlWqPsM/xnrJvcqYAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAg1VR7AHauUqlUewSgD3KmAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAINX0dGGlUtmbcwD7oFJKtUeglzlTACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkGqqPQDA3lCpVKo9wn7JmQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAFKllFKqPQQA+wZnCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCk/wOO+iVleEhbFgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAPTklEQVR4nO3baYhV9f/A8c+1UWd0/JXYaFoymmWFFZUR7dpi0rQ8iigi1IimSC0iLCLaNCLajLL1QYIEpdACYUmSUvogWixSFEU0KiE1zCLLYZzv/0HM5984rmVzzXm9YMA599xzP0flvuec+51KKaUEAEREj2oPAMDBQxQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRToEsOGDYuJEyfm94sXL45KpRKLFy+u2kw723nGrjB27Ng4+eSTD+gxq3EeHDpEoRuYPXt2VCqV/KqtrY2RI0fG5MmT48cff6z2ePtl/vz58dBDD1V1hkqlEpMnT67qDP+mRx99NK6++uoYNGhQVCqVqv9907Vqqj0AXeeRRx6J4cOHxx9//BFLliyJF198MebPnx/Lly+PPn36dOksF154Yfz+++/Rq1ev/Xre/PnzY9asWd6o/kX3339/HHXUUXH66afHggULqj0OXUwUupHLL788zjzzzIiIuPnmm2PAgAHx9NNPx7vvvhvXX3/9Lp/z22+/Rd++fQ/4LD169Ija2toDflz+uXXr1sWwYcNi8+bN0dDQUO1x6GJuH3VjF198cUT8+SYQETFx4sSor6+PtWvXRlNTU/Tr1y9uuOGGiIhoa2uLmTNnxqhRo6K2tjYGDRoUzc3NsWXLlg7HLKXEjBkz4phjjok+ffrERRddFCtWrOj02rv7TOHTTz+Npqam6N+/f/Tt2zdOPfXUePbZZ3O+WbNmRUR0uB3W7kDP+E+8++67ccUVV8SQIUOid+/eMWLEiJg+fXrs2LFjl/t/8cUXce6550ZdXV0MHz48XnrppU77bN++PR588ME47rjjonfv3jF06NCYNm1abN++fa/zrF27NtauXbtPsw8bNmyf9uPQ5EqhG2t/kxgwYEBua21tjfHjx8f5558fTz75ZN5Wam5ujtmzZ8ekSZNi6tSpsW7dunj++edj2bJlsXTp0ujZs2dERDzwwAMxY8aMaGpqiqampvjyyy/jsssui5aWlr3O8+GHH8aVV14ZgwcPjjvuuCOOOuqoWLlyZbz33ntxxx13RHNzc2zYsCE+/PDDmDNnTqfnd8WM+2r27NlRX18fd911V9TX18dHH30UDzzwQPzyyy/xxBNPdNh3y5Yt0dTUFNdee21cf/31MXfu3LjtttuiV69ecdNNN0XEn8G7+uqrY8mSJXHLLbfESSedFN98800888wzsXr16njnnXf2OM8ll1wSERHr168/YOfIIapwyHvttddKRJSFCxeWTZs2le+++6688cYbZcCAAaWurq58//33pZRSJkyYUCKi3HvvvR2e/8knn5SIKK+//nqH7R988EGH7Rs3biy9evUqV1xxRWlra8v97rvvvhIRZcKECblt0aJFJSLKokWLSimltLa2luHDh5fGxsayZcuWDq/z12PdfvvtZVf/bf+NGXcnIsrtt9++x322bdvWaVtzc3Pp06dP+eOPP3LbmDFjSkSUp556Krdt3769nHbaaWXgwIGlpaWllFLKnDlzSo8ePconn3zS4ZgvvfRSiYiydOnS3NbY2NjpPBobG0tjY+Nez+2vNm3aVCKiPPjgg/v1PP7b3D7qRi699NJoaGiIoUOHxnXXXRf19fXx9ttvx9FHH91hv9tuu63D9/PmzYvDDz88xo0bF5s3b86v0aNHR319fSxatCgiIhYuXBgtLS0xZcqUDrd17rzzzr3OtmzZsli3bl3ceeedccQRR3R47K/H2p2umHF/1NXV5Z9//fXX2Lx5c1xwwQWxbdu2WLVqVYd9a2pqorm5Ob/v1atXNDc3x8aNG+OLL77I8zvppJPixBNP7HB+7bcA289vd9avX+8qgX3i9lE3MmvWrBg5cmTU1NTEoEGD4oQTTogePTr+XFBTUxPHHHNMh21r1qyJrVu3xsCBA3d53I0bN0ZExLfffhsREccff3yHxxsaGqJ///57nK39VtbfXbPfFTPujxUrVsT9998fH330Ufzyyy8dHtu6dWuH74cMGdLpw/yRI0dGxJ9v5meffXasWbMmVq5cudsPftvPD/4pUehGzjrrrFx9tDu9e/fuFIq2trYYOHBgvP7667t8zsGwQuVgmvHnn3+OMWPGxP/+97945JFHYsSIEVFbWxtffvll3HPPPdHW1rbfx2xra4tTTjklnn766V0+PnTo0H86NkSEKLAPRowYEQsXLozzzjuvw22RnTU2NkbEnz+1H3vssbl906ZNnVYA7eo1IiKWL18el1566W73292tpK6YcV8tXrw4fvrpp3jrrbfiwgsvzO3tq7x2tmHDhk5Lf1evXh0R/78SaMSIEfH111/HJZdcsk+30+Dv8pkCe3XttdfGjh07Yvr06Z0ea21tjZ9//jki/vzMomfPnvHcc89FKSX3mTlz5l5f44wzzojhw4fHzJkz83jt/nqs9jfOnffpihn31WGHHdZp7paWlnjhhRd2uX9ra2u8/PLLHfZ9+eWXo6GhIUaPHh0Rf57fDz/8EK+++mqn5//+++/x22+/7XGm/VmSSvfmSoG9GjNmTDQ3N8djjz0WX331VVx22WXRs2fPWLNmTcybNy+effbZuOaaa6KhoSHuvvvueOyxx+LKK6+MpqamWLZsWbz//vtx5JFH7vE1evToES+++GJcddVVcdppp8WkSZNi8ODBsWrVqlixYkX+Zm37m+TUqVNj/Pjxcdhhh8V1113XJTP+1eeffx4zZszotH3s2LFx7rnnRv/+/WPChAkxderUqFQqMWfOnA6R+KshQ4bE448/HuvXr4+RI0fGm2++GV999VW88soruYz2xhtvjLlz58att94aixYtivPOOy927NgRq1atirlz58aCBQv2eGtwf5akzpkzJ7799tvYtm1bRER8/PHHea433nhjXm1xiKrq2ie6RPuS1M8++2yP+02YMKH07dt3t4+/8sorZfTo0aWurq7069evnHLKKWXatGllw4YNuc+OHTvKww8/XAYPHlzq6urK2LFjy/Llyzstk9x5SWq7JUuWlHHjxpV+/fqVvn37llNPPbU899xz+Xhra2uZMmVKaWhoKJVKpdPy1AM54+5ExG6/pk+fXkopZenSpeXss88udXV1ZciQIWXatGllwYIFnc55zJgxZdSoUeXzzz8v55xzTqmtrS2NjY3l+eef7/S6LS0t5fHHHy+jRo0qvXv3Lv379y+jR48uDz/8cNm6dWvu90+XpLYvk93V187/Xhx6KqXs5scXALodnykAkEQBgCQKACRRACCJAgBJFABI+/zLa361HjiQrIbftX/zvXZf/s5dKQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAKmm2gMAB69SSrVHOChVKpVqj/CvcaUAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAg1VR7AOCfKaVUe4SDUqVSqfYI/0muFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKAKSaag8A3UEppdojwD5xpQBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgCpptoDAN1TpVKp9gjsgisFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAqabaA8DBopRS7REOOpVKpdoj0MVcKQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAKmm2gPA/iilVHuEg06lUqn2CBxCXCkAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABINdUegENPKaXaIwB/kysFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAqabaA1AdpZRqj9CtVCqVao8A+8SVAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkGqqPQC7Vkqp9gjdTqVSqfYIUHWuFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACDVVHsA2B+VSqXaI8AhzZUCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAVFPtAf7LSinVHgHggHKlAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoApEoppVR7CAAODq4UAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEj/Bxpt34eiNuHQAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model4 = trained_models[0]\n",
    "\n",
    "def run_inference_on_images(model, images):\n",
    "    model.eval()  # sets model to evaluation mode\n",
    "    predictions = []\n",
    "    \n",
    "    for img in images:\n",
    "        # flatten each 16x16 image to a 1D tensor with 256 elements\n",
    "        input_data = torch.tensor(img.flatten(), dtype=torch.float32).unsqueeze(0).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        with torch.no_grad():  # disable gradient computation for inference\n",
    "            output = model(input_data)\n",
    "            _, predicted_label = torch.max(output, 1)  # get the predicted label\n",
    "            predictions.append(predicted_label.item())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "predictions = run_inference_on_images(trained_models[24], images)\n",
    "\n",
    "# displays some sample images with predictions\n",
    "for i in range(5):  \n",
    "    plt.imshow(images[i], cmap='gray')\n",
    "    plt.title(f\"Predicted Label: {predictions[i]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIFFLOGIC",
   "language": "python",
   "name": "difflogic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
